Paper ID	Created	Last Modified	Paper Title	Abstract	Primary Contact Author Name	Primary Contact Author Email	Authors	Author Names	Author Emails	Primary Subject Area	Secondary Subject Areas	Conflicts	Assigned	Completed	% Completed	Bids	Discussion	Status	Requested For Camera Ready	Camera Ready Submitted?	Requested For Author Feedback	Author Feedback Submitted?	Files	Number of Files	Supplementary Files	Number of Supplementary Files	Reviewers	Reviewer Emails	MetaReviewers	MetaReviewer Emails	SeniorMetaReviewers	SeniorMetaReviewerEmails	Q1 (Has this submission been published in an official conference proceedings? )
1	2/11/2021 4:18:52 PM -08:00	2/17/2021 8:35:53 PM -08:00	RobustPointSet: A Dataset for Benchmarking Robustness of Point Cloud Classifiers	The 3D deep learning community has seen significant strides in pointcloud processing over the last few years. However, the datasets on which deep models have been trained have largely remained the same. Most datasets comprise clean, clutter-free pointclouds canonicalized for pose. Models trained on these datasets fail in uninterpretible and unintuitive ways when presented with data that contains transformations ``unseen'' at train time. While data augmentation enables models to be robust to ``previously seen'' input transformations, 1) we show that this does not work for unseen transformations during inference, and 2) data augmentation makes it difficult to analyze a model's inherent robustness to transformations. To this end, we create a publicly available dataset for robustness analysis of point cloud classification models (independent of data augmentation) to input transformations, called RobustPointSet. Our experiments indicate that despite all the progress in the point cloud classification, there is no single architecture that consistently performs better---several fail drastically---when evaluated on transformed test sets. We also find that robustness to unseen transformations cannot be brought about merely by extensive data augmentation. RobustPointSet can be accessed through https://anonymous.	Saeid A Taghanaki	asgt.saeid@gmail.com	Saeid A Taghanaki (Autodesk)*; Jieliang Luo (Autodesk Research); Ran Zhang (Autodesk); Ye Wang (Autodesk Research); Pradeep Kumar Jayaraman (Autodesk Research); Krishna Murthy Jatavallabhula (Mila, DIRO, Universite de Montreal)	A Taghanaki, Saeid*; Luo, Jieliang; Zhang, Ran; Wang, Ye; Jayaraman, Pradeep Kumar; Jatavallabhula, Krishna Murthy	asgt.saeid@gmail.com*; rodger.luo@autodesk.com; ran.zhang@autodesk.com; ye.wang@autodesk.com; pradeep.kumar.jayaraman@autodesk.com; krrish94@gmail.com			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	RobustPointSet_ICRL2021_W.pdf (7,591,589 bytes)	1		0	Laurent Callot (Amazon Research); Mingjie Sun (Carnegie Mellon University)	lcallot@amazon.com; sunmj15@gmail.com	Kai-Wei Chang (UCLA)	kw@kwchang.net			
2	2/12/2021 6:43:43 PM -08:00	2/12/2021 7:07:08 PM -08:00	Hyperparameter Optimization Is Deceiving Us, and How to Stop It	While hyperparameter optimization (HPO) is known to greatly impact learning algorithm performance, it is often treated as an empirical afterthought. Recent empirical works have highlighted the risk of this second-rate treatment of HPO. They show that inconsistent performance results, based on choice of hyperparameter subspace to search, are a widespread problem in ML research. When comparing two algorithms, J and K, searching one subspace can yield the conclusion that J outperforms K, whereas searching another can entail the opposite result. In short, your choice of hyperparameters can deceive you. We provide a theoretical complement to this prior work: We analytically characterize this problem, which we term hyperparameter deception and show that grid search is inherently deceptive. We prove a defense with guarantees against deception, and demonstrate a defense in practice.	A. Feder Cooper	afc78@cornell.edu	A. Feder Cooper (Cornell University)*; Christopher De Sa (Cornell University); Yucheng Lu (Cornell University)	Cooper, A. Feder*; De Sa, Christopher; Lu, Yucheng	afc78@cornell.edu*; cmd353@cornell.edu; yl2967@cornell.edu			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	deception_workshop.pdf (404,284 bytes)	1		0	Li Erran Li (Amazon / Columbia University); Peter Hase (University of North Carolina at Chapel Hill)	erranlli@gmail.com; peter@cs.unc.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			
3	2/13/2021 4:36:28 AM -08:00	3/30/2021 9:50:09 AM -07:00	On the Benefits of Defining Vicinal Distributions in Latent Space	 The vicinal risk minimization (VRM) principle is an empirical risk minimization (ERM) variant that replaces Dirac masses with vicinal functions.  Mixup Training (MT), a popular choice of vicinal distribution, improves generalization performance of models by introducing globally linear behavior in between training examples. Apart from generalization, recent works have shown that mixup trained models are relatively robust to input perturbations/corruptions and at same time are calibrated better than their non-mixup counterparts. In this work, we investigate the benefits of defining these vicinal distributions like mixup in latent space of generative models rather than in input space itself. We propose a new approach - \textit{VarMixup (Variational Mixup)} - to better sample mixup images by using the latent manifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100 and Tiny-ImageNet demonstrates  that  models trained by performing mixup in the latent manifold  learned  by  VAEs  are  inherently  more  robust to various input corruptions and are significantly better calibrated than vanilla mixup.	Puneet Mangla	cs17btech11029@iith.ac.in	Puneet Mangla (IIT Hyderabad)*; Vedant Singh (IIT, Hyderabad); Shreyas Havaldar (Indian Institute of Technology Hyderabad); Vineeth Balasubramanian (Indian Institute of Technology Hyderabad)	Mangla, Puneet*; Singh, Vedant; Havaldar, Shreyas; Balasubramanian, Vineeth	cs17btech11029@iith.ac.in*; cs18btech11047@iith.ac.in; cs18btech11042@iith.ac.in; vineethnb@cse.iith.ac.in			0	2	2	100	0	Disabled (0)	Accept (oral)	Yes	Yes	No	No	VarMixup.pdf (608,241 bytes)	1		0	Chris Clark (UW); Han Guo (CMU)	csquared@cs.washington.edu; hanguo@cs.cmu.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			
4	2/13/2021 10:40:21 AM -08:00	2/26/2021 12:27:14 PM -08:00	Defending Against Image Corruptions Through Adversarial Augmentations	Modern neural networks excel at image classification, achieving superhuman performance, yet they remain vulnerable to common image corruptions such as blur, speckle noise or fog. Recent methods that focus on this problem, such as AugMix and DeepAugment, introduce  defenses that operate in expectation over a distribution of image corruptions. In contrast, the literature on Lp-norm bounded perturbations focuses on defenses against worst-case corruptions. In this work, we reconcile both approaches by proposing AdversarialAugment, a technique which optimizes the parameters of image-to-image models to generate adversarially corrupted augmented images. Our classifiers improve upon the state-of-the-art on common image corruption benchmarks conducted in expectation on CIFAR-10-C and improve worst-case performance against Lp-norm bounded perturbations.	Dan Andrei Calian	dancalian@google.com	Dan Andrei Calian (DeepMind)*; Florian Stimberg (); Sylvestre-Alvise Rebuffi (DeepMind); Olivia Wiles (DeepMind); Andras Gyorgy (DeepMind); Timothy  Arthur Mann (DeepMind); Sven Gowal (DeepMind)	Calian, Dan Andrei*; Stimberg, Florian; Rebuffi, Sylvestre-Alvise; Wiles, Olivia; Gyorgy, Andras; Mann, Timothy  Arthur; Gowal, Sven	dancalian@google.com*; stimberg@google.com; sylvestre@google.com; oawiles@google.com; agyorgy@google.com; timothymann@google.com; sgowal@google.com			3	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Defending_Against_Image_Corruptions_Through_Adversarial_Augmentations.pdf (1,614,739 bytes)	1		0	Francesco Croce (University of Tübingen); Sailik Sengupta (Arizona State University)	francesco91.croce@gmail.com; sailiks@asu.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			No.
5	2/17/2021 2:14:46 AM -08:00	2/18/2021 4:49:54 AM -08:00	Constant Random Perturbations Provide Adversarial Robustness with Minimal Effect on Accuracy	This paper proposes an attack-independent (non-adversarial training) technique for improving adversarial robustness of neural network models, with minimal loss of standard accuracy. We suggest creating a neighborhood around each training example, such that the label is kept constant for all inputs within that neighborhood. Unlike previous work that follows a similar principle, we apply this idea by extending the training set with multiple perturbations for each training example, drawn from within the neighborhood. These perturbations are model independent, and remain constant throughout the entire training process. We analyzed our method empirically on MNIST, SVHN, and CIFAR-10, under different attacks and conditions. Results suggest that the proposed approach improves standard accuracy over other defenses while having increased robustness compared to vanilla adversarial training.	Bronya Roni Chernyak	chernroni@gmail.com	Bronya Roni Chernyak (Bar Ilan University)*; Bhiksha Raj (Carnegie Mellon University); Tamir Hazan (Technion); Joseph Keshet (Dept. of Computer Science, Bar-Ilan University)	Chernyak, Bronya Roni*; Raj, Bhiksha; Hazan, Tamir; Keshet, Joseph	chernroni@gmail.com*; bhiksha@cs.cmu.edu; tamir.hazan@technion.ac.il; jkeshet@cs.biu.ac.il			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	CRP_Provide_Adversarial_Robustness_with_Minimal_Effect_on_Accuracy_WORKSHOP_ICLR_2021.pdf (402,610 bytes)	1		0	Baoyuan Wu (The Chinese University of Hong Kong, Shenzhen); Dan Andrei Calian (DeepMind)	wubaoyuan@cuhk.edu.cn; dancalian@google.com	Kai-Wei Chang (UCLA)	kw@kwchang.net			
6	2/18/2021 12:35:04 PM -08:00	2/25/2021 12:58:22 PM -08:00	FINE-GRAINED Ɛ-MARGIN CLOSED-FORM STABILIZATION OF PARAMETRIC HAWKES PROCESSES	Hawkes Processes have undergone increasing popularity as default tools for modeling self- and mutually exciting interactions of discrete events in continuous-time event streams. A Maximum Likelihood Estimation (MLE) unconstrained optimization procedure over parametrically assumed forms of the triggering kernels of the corresponding intensity function are a widespread cost-effective modeling strategy, particularly suitable for data with few and/or short sequences. However, the MLE optimization lacks guarantees, except for strong assumptions on the parameters of the triggering kernels, and may lead to instability of the resulting parameters . In the present work, we show how a simple stabilization procedure improves the performance of the MLE optimization without these overly restrictive assumptions. This stabilized version of the MLE is shown to outperform traditional methods over sequences of several different lengths.	Rafael G de Lima	rg.lima@samsung.com	Rafael G de Lima (Samsung Electronics)*	Lima, Rafael G de*	rg.lima@samsung.com*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	HP_Workshop_Submission.pdf (350,533 bytes)	1		0	Kuan-Hao Huang (University of California, Los Angeles); Mahyar Fazlyab (Johns Hopkins University)	khhuang@cs.ucla.edu; mahyarfazlyab@jhu.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			
7	2/20/2021 10:43:29 AM -08:00	2/26/2021 2:15:43 AM -08:00	Bit Error Robustness for Energy-Efficient DNN Accelerators	Deep neural network (DNN) accelerators received considerable attention in past years due to saved energy compared to mainstream hardware. Low-voltage operation of DNN accelerators allows to further reduce energy consumption significantly, however, causes bit-level failures in the memory storing the quantized DNN weights. In this paper, we show that a combination of robust fixed-point quantization, weight clipping, and random bit error training (RandBET) improves robustness against random bit errors in (quantized) DNN weights significantly. This leads to high energy savings from both low-voltage operation as well as low-precision quantization. Furthermore, our approach generalizes across operating voltages and accelerators, as demonstrated on bit errors from profiled SRAM arrays. Without losing more than 1% in accuracy, we can reduce energy consumption on CIFAR10 by 20% for a 8-bit quantized DNN. Higher energy savings of, e.g., 30%, are possible at the cost of 2.5% accuracy, even for 4-bit DNNs.  	David Stutz	david.stutz@mpi-inf.mpg.de	David Stutz (Max Planck Institute for Informatics)*; Nandhini Chandramoorthy (IBM T. J. Watson Research Center); Matthias Hein (University of Tübingen); Bernt Schiele (MPI Informatics)	Stutz, David*; Chandramoorthy, Nandhini; Hein, Matthias; Schiele, Bernt	david.stutz@mpi-inf.mpg.de*; Nandhini.Chandramoorthy@ibm.com; matthias.hein@uni-tuebingen.de; schiele@mpi-inf.mpg.de			2	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	submission-7.pdf (692,738 bytes)	1		0	Ammar Shaker (NEC Laboratories Europe); Swarnadeep Saha (UNC Chapel Hill)	ammar.shaker@neclab.eu; swarna@cs.unc.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			Accepted at the Third Conference on Machine Learning and Systems (MLSys) 2021, see mlsys.org.
9	2/22/2021 12:47:34 PM -08:00	3/6/2021 3:37:34 AM -08:00	On Pathologies in KL-Regularized Reinforcement Learning from Expert Demonstrations	KL-regularized reinforcement learning from expert demonstrations has proved successful in improving the sample efficiency of deep reinforcement learning algorithms, allowing them to be applied to challenging physical real-world tasks. However, we show that KL-regularized reinforcement learning with behavioral policies derived from expert demonstrations suffers from hitherto unrecognized pathological behavior that can lead to slow, unstable, and suboptimal online training. We show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efficiency and online policy performance. Finally, we show that the pathology can be remedied by specifying non-parametric behavioral policies and that doing so allows KL-regularized RL to significantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks---without ad-hoc algorithmic design choices.	Tim G. J. Rudner	tim.rudner@cs.ox.ac.uk	Tim G. J. Rudner (University of Oxford)*; Cong Lu (University of Oxford); Michael A.  Osborne (University of Oxford); Yarin Gal (University of Oxford); Yee Whye Teh (University of Oxford)	Rudner, Tim G. J.*; Lu, Cong; Osborne, Michael A. ; Gal, Yarin; Teh, Yee Whye	tim.rudner@cs.ox.ac.uk*; cong.lu@balliol.ox.ac.uk; mosb@robots.ox.ac.uk; yarin@cs.ox.ac.uk; y.w.teh@stats.ox.ac.uk			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	On_Pathologies_in_KL_Regularized_Reinforcement_Learning_from_Expert_Demonstrations.pdf (4,229,581 bytes)	1		0	Jessica Yung (Google Research); zhiting hu (Amazon / UC San Diego)	j.yung357@gmail.com; zhitinghu@gmail.com	Kai-Wei Chang (UCLA)	kw@kwchang.net			
10	2/22/2021 1:13:26 PM -08:00	2/25/2021 9:37:11 AM -08:00	An Online Learning Approach to Interpolation and Extrapolation in Domain Generalization	A popular assumption for out-of-distribution generalization is that the training data comprises sub-datasets, each drawn from a distinct distribution; the goal is then to ``interpolate'' these distributions and ``extrapolate'' beyond them---this objective is broadly known as domain generalization. A common belief is that ERM can interpolate but not extrapolate and that the latter is considerably more difficult, but these claims are vague and lack formal justification. In this work, we recast generalization over sub-groups as an online game between a player minimizing risk and an adversary presenting new test distributions. Under an existing notion of inter- and extrapolation based on reweighting of sub-group likelihoods, we rigorously demonstrate that extrapolation is computationally much harder than interpolation, though their statistical complexity is not significantly different. Furthermore, we show that ERM---or a noisy variant---is \emph{provably minimax-optimal} for both tasks. Our framework presents a new avenue for the formal analysis of domain generalization algorithms which may be of independent interest.	Elan Rosenfeld	ekr@andrew.cmu.edu	Elan Rosenfeld (Carnegie Mellon University)*; Pradeep Ravikumar (Carnegie Mellon University); Andrej Risteski (CMU)	Rosenfeld, Elan*; Ravikumar, Pradeep; Risteski, Andrej	ekr@andrew.cmu.edu*; pradeepr@cs.cmu.edu; aristesk@andrew.cmu.edu			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	An_Online_Learning_Approach_to_Interpolation_and_Extrapolation_in_Domain_Generalization.pdf (253,662 bytes)	1		0	Deyi Liu (University of North Carolina); Mahyar Fazlyab (Johns Hopkins University)	deyi@live.unc.edu; mahyarfazlyab@jhu.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			
11	2/23/2021 1:33:36 AM -08:00	2/26/2021 2:33:34 PM -08:00	Robust Learning-Augmented Caching: An Experimental Study	Effective caching is crucial for the performance of modern-day computing systems. A key optimization problem arising in caching -- which item to evict to make room for a new item -- cannot be optimally solved without knowing the future. There are many classical approximation algorithms for this problem, but more recently, researchers started to successfully apply machine learning (ML) to decide what to evict by discovering implicit input patterns and predicting the future. While ML typically does not provide any worst-case guarantees, the new field of learning-augmented algorithms proposes solutions that leverage classical online caching algorithms to make the ML predictors robust. We are the first to comprehensively evaluate these learning-augmented algorithms on real-world caching datasets and state-of-the-art ML predictors. We show that a straightforward method -- blindly following either a predictor or a classical robust algorithm, and switching whenever one becomes worse than the other -- has only a low overhead over a well-performing predictor, while competing with classical methods when the coupled predictor fails, thus providing a cheap worst-case insurance.	Adam Polak	adam.polak@epfl.ch	Jakub Chłędowski (Jagiellonian University); Adam Polak (École Polytechnique Fédérale de Lausanne)*; Bartosz Szabucki (Jagiellonian University); Konrad Tomasz Żołna (Jagiellonian University)	Chłędowski, Jakub; Polak, Adam*; Szabucki, Bartosz; Żołna, Konrad Tomasz	jakub.chledowski@gmail.com; adam.polak@epfl.ch*; bartosz.szabucki@gmail.com; konrad.zolna@gmail.com			9	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	paper.pdf (285,608 bytes)	1		0	Jwala Dhamala (Amazon Alexa AI); Swarnadeep Saha (UNC Chapel Hill)	jwaladhamala@gmail.com; swarna@cs.unc.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			No
12	2/23/2021 7:50:49 AM -08:00	2/23/2021 7:50:49 AM -08:00	Out-of-distribution detection in satellite image classification	In satellite image analysis, distributional mismatch between the training and test data may arise due to several reasons, including unseen classes in the test data and differences in the geographic area. Deep learning based models may behave in unexpected manner when subjected to test data that has such distributional shifts from the training data, also called out-of-distribution (OOD) examples. Predictive uncertainly analysis is an emerging research topic which has not been explored much in context of satellite image analysis.  Towards this, we adopt a  Dirichlet Prior Network based  model to quantify distributional uncertainty of deep learning models for remote sensing. The approach seeks to maximize the representation gap between the in-domain and OOD examples for a better identification of unknown examples at test time. Experimental results on three exemplary test scenarios show the efficacy of the model  in satellite image analysis.	Sudipan Saha	sudipan.saha@tum.de	Jakob Gawlikowski (German Aerospace Center (DLR)); Sudipan Saha (Technical University of Munich)*; Anna Kruspe (Technical University of Munich); Xiaoxiang Zhu (Technical University of Munich (TUM); German Aerospace Center (DLR))	Gawlikowski, Jakob; Saha, Sudipan*; Kruspe, Anna; Zhu, Xiaoxiang	Jakob.Gawlikowski@dlr.de; sudipan.saha@tum.de*; anna.kruspe@tum.de; xiaoxiang.zhu@dlr.de			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	oodIclrwRound1.pdf (283,238 bytes)	1		0	Sameer  Singh (University of California, Irvine); Swarnadeep Saha (UNC Chapel Hill)	sameer@uci.edu; swarna@cs.unc.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			No
13	2/23/2021 10:24:40 PM -08:00	2/26/2021 9:15:06 PM -08:00	Class-Imbalanced Semi-Supervised Learning	Semi-Supervised Learning has achieved great success in overcoming the difficulties of labeling and making full use of unlabeled data.However, Semi-Supervised Learning has a limited assumption that the numbers of samples in different classes are balanced, and many Semi-Supervised Learning algorithms show lower performance for the datasets with the imbalanced class distribution. In this paper, we introduce a task of Class-Imbalanced Semi-Supervised Learning, which refers to semi-supervised learning with class-imbalanced data. In doing so, we consider class imbalance in both labeled and unlabeled sets. We propose Suppressed Consistency Loss, a regularization method robust to class imbalance. Our method shows better performance than the conventional methods in the Class-Imbalanced Semi-Supervised Learning task. In particular, the more severe the class imbalance and the smaller the size of the labeled data, the better our method performs.	Nojun Kwak	nojunk@snu.ac.kr	Minsung Hyun (Seoul National University); Jisoo Jeong (Seoul National University); Nojun Kwak (Seoul National University)*	Hyun, Minsung; Jeong, Jisoo; Kwak, Nojun*	minsung.hyun@snu.ac.kr; soo3553@snu.ac.kr; nojunk@snu.ac.kr*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	iclr2021_robustML_workshop_Class_Imbalanced_Semi_Supervised_Learning.pdf (664,364 bytes)	1		0	Zhenlin Xu (UNC Chapel Hill); zhiting hu (Amazon / UC San Diego)	zhenlinx@cs.unc.edu; zhitinghu@gmail.com	Kai-Wei Chang (UCLA)	kw@kwchang.net			No
14	2/23/2021 11:55:15 PM -08:00	2/23/2021 11:55:15 PM -08:00	Disparate impact when learning with noisy labels	This paper aims to provide understandings for the effect of an over-parameterized model, e.g. a deep neural network, memorizing instance-dependent noisy labels. We first quantify the harms caused by memorizing noisy instances from different spectra of the sample distribution. We then analyze how several popular solutions for learning with noisy labels mitigate this harm at the instance-level. Our analysis reveals that existing approaches handle noisy instances differently. While higher-frequency instances often enjoy a high probability of an improvement by applying these approaches, lower-frequency instances do not. This observation requires us to rethink the distribution of label noise across instances and might potentially require different treatments for instances in different regimes.	Yang Liu	yangliu@ucsc.edu	Yang Liu (UC Santa Cruz)*	Liu, Yang*	yangliu@ucsc.edu*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	RML_Disparate_Final.pdf (1,837,359 bytes)	1		0	Deyi Liu (University of North Carolina); Shibani Santurkar (MIT)	deyi@live.unc.edu; shibani@mit.edu	Yonatan Belinkov (Technion)	belinkov@mit.edu			No.
15	2/24/2021 8:06:38 PM -08:00	3/1/2021 7:37:32 PM -08:00	Interpretable Mixture Density Estimation by use of Differentiable Tree-module	In order to develop reliable services using machine learning, it is important to understand the uncertainty of the model outputs. Often the probability distribution that the prediction target follows has a complex shape, and a mixture distribution is assumed as a distribution that uncertainty follows. Since the output of mixture density estimation is complicated, its interpretability becomes important when considering its use in real services. In this paper, we propose a method for mixture density estimation that utilizes an interpretable tree structure. Further, a fast inference procedure based on time-invariant information cache achieves both high speed and interpretability.	Ryuichi Kanoh	kanoh.ryuichi@gmail.com	Ryuichi Kanoh (National Institute of Informatics, The Graduate University for Advanced Studies (SOKENDAI))*; Tomu Yanabe (Keio University)	Kanoh, Ryuichi*; Yanabe, Tomu	kanoh.ryuichi@gmail.com*; yanabekaka@gmail.com			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	iclrws.pdf (432,555 bytes)	1		0	Adyasha Maharana (UNC Chapel Hill); Sameer  Singh (University of California, Irvine)	adyasha@cs.unc.edu; sameer@uci.edu	Yonatan Belinkov (Technion)	belinkov@mit.edu			No
16	2/25/2021 12:20:50 AM -08:00	3/27/2021 6:39:44 PM -07:00	Backdoor Attack in the Physical World	Backdoor attack intends to inject hidden backdoor into the deep neural networks (DNNs), such that the prediction of infected models will be maliciously changed if the hidden backdoor is activated by the attacker-defined trigger. Currently, most existing backdoor attacks adopted the setting of \emph{static} trigger, $i.e.,$ triggers across the training and testing images follow the same appearance and are located in the same area. In this paper, we revisit this attack paradigm by analyzing trigger characteristics. We demonstrate that this attack paradigm is vulnerable when the trigger in testing images is not consistent with the one used for training. As such, those attacks are far less effective in the physical world, where the location and appearance of the trigger in the digitized image may be different from that of the one used for training. Moreover, we also discuss how to alleviate such vulnerability. We hope that this work could inspire more explorations on backdoor properties, to help the design of more advanced backdoor attack and defense methods.	Yiming Li	li-ym18@mails.tsinghua.edu.cn	Yiming Li (Tsinghua University)*; Tongqing Zhai (Tsinghua University); Yong Jiang (Tsinghua University); Zhifeng Li (Tencent AI Lab); Shutao Xia (Tsinghua University)	Li, Yiming*; Zhai, Tongqing; Jiang, Yong; Li, Zhifeng; Xia, Shutao	li-ym18@mails.tsinghua.edu.cn*; dtq18@mails.tsinghua.edu.cn; jiangy@sz.tsinghua.edu.cn; michaelzfli@tencent.com; xiast@sz.tsinghua.edu.cn			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Backdoor_ICLRW2021.pdf (1,637,367 bytes)	1		0	Apostolos Modas (EPFL); Seyed-Mohsen Moosavi-Dezfooli (ETH Zurich)	apostolos.modas@epfl.ch; seyed.moosavi@inf.ethz.ch	Yonatan Belinkov (Technion)	belinkov@mit.edu			No, but its extension version has already been posted on arXiv.
18	2/25/2021 3:32:35 AM -08:00	3/1/2021 12:19:23 AM -08:00	Adversarial training may be a double-edged sword	Adversarial training has been shown as an effective approach to improve the robustness of image classifiers against white-box attacks. However, its effectiveness against black-box attacks is more nuanced.  In this work, we demonstrate that some geometric consequences of adversarial training on the decision boundary of deep networks give an edge to certain types of black-box attacks. In particular, we define a metric called robustness gain to  show that while adversarial training is an effective method to dramatically improve the robustness in  white-box scenarios, it may not provide such a good robustness gain against the more realistic decision-based black-box attacks. Moreover, we show that even the minimal perturbation white-box  attacks   can converge faster against adversarially-trained neural networks compared to the regular ones.	Ali Rahmati	arahmat@ncsu.edu	Ali Rahmati (NC State University)*; Seyed-Mohsen Moosavi-Dezfooli (ETH Zurich); Huaiyu Dai (NC State University)	Rahmati, Ali*; Moosavi-Dezfooli, Seyed-Mohsen; Dai, Huaiyu	arahmat@ncsu.edu*; seyed.moosavi@inf.ethz.ch; hdai@ncsu.edu			4	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	18.pdf (193,961 bytes)	1		0	Michael Everett (MIT); Philipp Benz (KAIST)	mfe@mit.edu; pbenz@kaist.ac.kr	Yonatan Belinkov (Technion)	belinkov@mit.edu			No
19	2/25/2021 5:11:31 AM -08:00	3/5/2021 11:20:54 PM -08:00	Reclaiming uncertainties of deterministic deep models with Vine Copulas	Despite the major progress of deep models as learning machines, uncertainty estimation remains a major challenge. Existing solutions rely on modified loss functions or architectural changes. We propose to compensate for the lack of built-in uncertainty estimates by supplementing any network, retrospectively, with a subsequent vine copula model, Vine-Copula Neural Networks (VCNN).  Through synthetic and real-data experiments, we show that VCNNs could be task (regression/classification) and architecture (recurrent, fully connected) agnostic, with uncertainty estimates comparable to state-of-the-art solutions.	Natasa Tagasovska	natasa.tagasovska@epfl.ch	Natasa Tagasovska (EPFL)*; Axel Brando (Universitat de Barcelona (UB) and Barcelona Supercomputing Center (BSC)); Firat Ozdemir ()	Tagasovska, Natasa*; Brando, Axel; Ozdemir, Firat	natasa.tagasovska@epfl.ch*; axelbrando@ub.edu; Firat.Ozdemir@sdsc.ethz.ch			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Reclaiming_uncertainties_of_deterministic_deep_models_with_Vine_Copulas.pdf (825,733 bytes)	1		0	Bhavya Kailkhura (Lawrence Livermore National Laboratory); Payel Das (IBM Research)	kailkhura1@llnl.gov; daspa@us.ibm.com	Yonatan Belinkov (Technion)	belinkov@mit.edu			
20	2/25/2021 5:36:19 AM -08:00	2/26/2021 4:13:20 AM -08:00	An Infinite-Feature Extension for Bayesian ReLU Nets That Fixes Their Asymptotic Overconfidence	A Bayesian treatment can mitigate overconfidence in ReLU nets around the training data. But far away from them, ReLU Bayesian neural networks (BNNs) can still underestimate uncertainty and thus be asymptotically overconfident. This issue arises since the output variance of a BNN with finitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models with ReLU features converge, in the infinite-width limit, to a particular Gaussian process (GP) with a variance that grows cubically so that no asymptotic overconfidence can occur. While this may seem of mostly theoretical interest, in this work, we show that it can be used concretely to the benefit of BNNs. We extend finite ReLU BNNs with infinite ReLU features via the GP and show that the resulting model is asymptotically maximally uncertain far away from the data while the BNNs' predictive power is unaffected near the data. Although the resulting model approximates a full GP posterior, thanks to its structure it can be applied \emph{post-hoc} to any pre-trained ReLU BNN at a low cost.	Agustinus Kristiadi	agustinus.kristiadi@uni-tuebingen.de	Agustinus Kristiadi (University of Tübingen)*; Matthias Hein (University of Tübingen); Philipp Hennig (University of Tübingen and MPI for Intelligent Systems Tübingen)	Kristiadi, Agustinus*; Hein, Matthias; Hennig, Philipp	agustinus.kristiadi@uni-tuebingen.de*; matthias.hein@uni-tuebingen.de; ph@tue.mpg.de			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	main.pdf (515,102 bytes)	1		0	Julius Adebayo (MIT); Shibani Santurkar (MIT)	juliusad@mit.edu; shibani@mit.edu	Yonatan Belinkov (Technion)	belinkov@mit.edu			
21	2/25/2021 5:56:51 AM -08:00	2/25/2021 7:22:23 AM -08:00	Invariant risk minimization for natural language inference	Following the recent discovery that many natural language inference (NLI) models rely on biased features to make their predictions, several approaches have been suggested to mitigating bias. Many of them are designed for a specific a priori known bias and are not easily extendable to other types of biases. In addition, approaches concerning representation debiasing often require access to a specific representation layer, thus are possibly not scalable to other models. In this work we explore the applicability of a recently proposed method called Invariant Risk Minimization (IRM) to overcome these issues. IRM is a model-agnostic training scheme that suggests to look at ”unshuffled” subsets of the training data to discover stable rather than spurious correlations. We examine possible ways to generate such subsets to debias an NLI model and run several experiments. Our results show that as we proceed to more natural settings, IRM displays unstable performance. While in some use cases it outperforms Empirical Risk Minimization (ERM) on the out of distribution (OOD) data, further work needs to be done to enable its application as a practical debiasing method.	Yana Dranker	yanadr@campus.technion.ac.il	Yana Dranker (Technion)*; Yonatan Belinkov (Technion)	Dranker, Yana*; Belinkov, Yonatan	yanadr@campus.technion.ac.il*; belinkov@technion.ac.il			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR_2021_workshop.pdf (674,707 bytes)	1		0	Javid Ebrahimi (Visa); Xiaoqing Zheng (Fudan University)	jebivid@gmail.com; zhengxq@fudan.edu.cn	Di Jin (MIT)	jindi15@mit.edu			
22	2/25/2021 6:25:46 AM -08:00	2/25/2021 6:25:46 AM -08:00	A Generative Approach for Mitigating Hypothesis-Only Biases in Natural Language Inference	Natural Language Inference (NLI) is one of the key tasks in Natural Language Processing (NLP), where a model is required to predict the relationship between two sentences, a premise, and a hypothesis.  Recent studies point out that much of the NLI datasets contained biases that allowed the models to perform well by only seeing the hypothesis, without learning how the sentences relate to each other.  We propose a generative model, that generates the premise given the hypothesis and the label.  We find that this approach leads to unbiased models for out-of-distribution data.  However, we have found that generative models are difficult to train and that they perform worse than discriminative models in general. 	Dimion Asael	dimion@cs.technion.ac.il	Dimion Asael (Technion)*; Yonatan Belinkov (Technion)	Asael, Dimion*; Belinkov, Yonatan	dimion@cs.technion.ac.il*; belinkov@technion.ac.il			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	RobustML_Workshop_ICLR_2021_submission.pdf (220,796 bytes)	1		0	Adyasha Maharana (UNC Chapel Hill); Han Guo (CMU)	adyasha@cs.unc.edu; hanguo@cs.cmu.edu	Di Jin (MIT)	jindi15@mit.edu			
23	2/25/2021 9:00:12 AM -08:00	3/5/2021 10:01:53 AM -08:00	Generating Adversarial Examples with Graph Neural Networks	Recent years have witnessed the deployment of adversarial attacks to evaluate the robustness of Neural Networks. Past work in this field has relied on traditional optimization algorithms that ignore the inherent structure of the problem and data, or generative methods that rely purely on learning and often fail to generate adversarial examples where they are hard to find. To alleviate these deficiencies, we propose a novel attack based on a graph neural network (GNN) that takes advantage of the strengths of both approaches; we call it AdvGNN. Our GNN architecture closely resembles the network we wish to attack.  During inference, we perform forward-backward passes through the GNN layers to guide an iterative procedure towards adversarial examples. We show that our method beats state-of-the-art adversarial attacks, reducing the time required to generate adversarial examples with small perturbation norms by over 65\% while also achieving good generalization performance on unseen networks.	Florian Jaeckle	florian.jaeckle@gmail.com	Florian Jaeckle (University of Oxford)*; M. Pawan Kumar (University of Oxford)	Jaeckle, Florian*; Kumar, M. Pawan	florian.jaeckle@gmail.com*; pawan@robots.ox.ac.uk			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	 RobustML_workshop_ICLR2021_(1).pdf (653,846 bytes)	1		0	Maksym Andriushchenko (EPFL); Philipp Benz (KAIST)	maksym.andriushchenko@epfl.ch; pbenz@kaist.ac.kr	Yonatan Belinkov (Technion)	belinkov@mit.edu			
24	2/25/2021 9:21:48 AM -08:00	2/26/2021 10:04:02 AM -08:00	Diagnosing Vulnerability of Variational Auto-Encoders to Adversarial Attacks	In this work, we explore adversarial attacks on the Variational Autoencoders (VAE). We show how to modify data point to obtain a prescribed latent code (supervised attack) or just get a drastically different code (unsupervised attack). We examine the influence of model modifications ($\beta$-VAE, NVAE) on the robustness of VAEs and suggest metrics to quantify it.	Anna Kuzina	a.kuzina@vu.nl	Anna Kuzina (Vrije Universiteit Amsterdam)*; Max Welling (University of Amsterdam); Jakub M Tomczak (Vrije Universiteit Amsterdam)	Kuzina, Anna*; Welling, Max; Tomczak, Jakub M	a.kuzina@vu.nl*; welling.max@gmail.com; jmk.tomczak@gmail.com			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	VAE_adversarial.pdf (1,515,566 bytes)	1		0	Bhavya Kailkhura (Lawrence Livermore National Laboratory); Michael Everett (MIT)	kailkhura1@llnl.gov; mfe@mit.edu	Yonatan Belinkov (Technion)	belinkov@mit.edu			
25	2/25/2021 9:24:06 AM -08:00	2/25/2021 9:24:06 AM -08:00	On Pitfalls of Measuring Occlusion Robustness through Data Distortion	Over the past years, the crucial role of data has largely been shadowed by the field's focus on architectures and training procedures. We often cause changes to the data without being aware of their wider implications. In this paper we show that distorting images without accounting for the artefacts introduced leads to biased results when establishing occlusion robustness. To ensure models behave as expected in real-world scenarios, we need to rule out the impact added artefacts have on evaluation. We propose a new method, iOcclusion, as a fairer alternative measure for applications where the possible occluders are unknown.	Antonia Marcu	am1g15@soton.ac.uk	Antonia Marcu (University of Southampton)*	Marcu, Antonia*	am1g15@soton.ac.uk*			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Robust_ML_____ICLR_workshop.pdf (453,838 bytes)	1		0	Saeid Asgari Taghanaki (Autodesk AI Lab); Xiaoqing Zheng (Fudan University)	saeid.asgari.taghanaki@autodesk.com; zhengxq@fudan.edu.cn	Yonatan Belinkov (Technion)	belinkov@mit.edu			No, but it is under review for ICML 2021.
26	2/25/2021 9:54:15 AM -08:00	2/26/2021 5:30:46 AM -08:00	Helper-based Adversarial Training: Preventing Invariance to Boost Accuracy	While adversarial training has become the de facto approach for training robust classifiers, it leads to a drop in standard accuracy. This has led to prior works postulating that accuracy is at inherent odds with robustness. Yet, the phenomenon remains inexplicable. In this paper, we closely examine the changes induced in the decision boundary of a deep network during adversarial training. We find that adversarial training leads to excessive invariance along certain adversarial directions, thereby hurting accuracy. Lastly, we present a simple algorithm, called Helper-based Adversarial Training (HAT), to mitigate this excessive invariance. Our proposed method achieves a significant improvement in standard accuracy without compromising robustness.	Rahul Shekhar Rade	rarade@student.ethz.ch	Rahul Shekhar Rade (ETH Zurich)*; Seyed-Mohsen Moosavi-Dezfooli (ETH Zurich)	Rade, Rahul Shekhar*; Moosavi-Dezfooli, Seyed-Mohsen	rarade@student.ethz.ch*; seyed.moosavi@inf.ethz.ch			4	2	1	50	0	Disabled (0)	Withdrawn	Yes	No	No	No	helper-based-adversarial-training.pdf (2,702,406 bytes)	1		0	Krishnamurthy Dvijotham (DeepMind); Pratyush Maini (IIT Delhi)	dvij@google.com; pratyush.maini@gmail.com	Yonatan Belinkov (Technion)	belinkov@mit.edu			
27	2/25/2021 12:02:10 PM -08:00	2/25/2021 12:03:33 PM -08:00	Just Train Twice: Improving Group Robustness Without Training Group Information	Standard training via empirical risk minimization (ERM) can produce models that achieve low error on average but high error on minority groups, especially in the presence of spurious correlations between the input and label. Prior approaches to this problem, like group distributionally robust optimization (group DRO), generally require group annotations for every training point. On the other hand, approaches that do not use group annotations generally do not improve minority performance. For example, we find that joint DRO, which dynamically upweights examples with high training loss, tends to optimize for examples that are irrelevant to the specific groups we seek to do well on. In this paper, we propose a simple two-stage approach, JTT, that achieves comparable performance to group DRO while only requiring group annotations on a significantly smaller validation set. JTT first attempts to identify informative training examples, which are often minority examples, by training an initial ERM classifier and selecting the examples with high training loss. Then, it trains a final classifier by upsampling the selected examples. On four image classification and natural language processing tasks with spurious correlations, we show that JTT closes 85% of the gap in accuracy on the worst group between ERM and group DRO.	Evan Liu	evanliu@cs.stanford.edu	Evan Liu (Stanford University)*; Behzad Haghgoo (Stanford University); Annie S Chen (Stanford University); Aditi Raghunathan (Stanford University); Pang Wei Koh (Stanford University); Shiori Sagawa (Stanford University); Percy Liang (Stanford University); Chelsea Finn (Stanford)	Liu, Evan*; Haghgoo, Behzad; Chen, Annie S; Raghunathan, Aditi; Koh, Pang Wei; Sagawa, Shiori; Liang, Percy; Finn, Chelsea	evanliu@cs.stanford.edu*; bhaghgoo@stanford.edu; asc8@stanford.edu; aditir@stanford.edu; pangwei@cs.stanford.edu; ssagawa@cs.stanford.edu; pliang@cs.stanford.edu; cbfinn@cs.stanford.edu			2	2	1	50	0	Disabled (0)	Accept	Yes	No	No	No	spurious_iclrWS_final.pdf (1,280,494 bytes)	1		0	Shujian Yu (NEC Laboratories Europe); Xiaoqing Zheng (Fudan University)	Shujian.Yu@neclab.eu; zhengxq@fudan.edu.cn	Yonatan Belinkov (Technion)	belinkov@mit.edu			Has not been published in an official conference proceedings.
29	2/25/2021 2:43:40 PM -08:00	2/26/2021 5:08:11 PM -08:00	Gi and Pal Scores: Deep Neural Network Generalization Statistics	The field of Deep Learning is rich with empirical evidence of human-like performance on a variety of regression, classification, and control tasks. However, despite these successes, the field lacks strong theoretical error bounds and consistent measures of network generalization and learned invariances. In this work, we introduce two new measures, the Gi-score and Pal-score, that capture a deep neural network's generalization capabilities. Inspired by the Gini coefficient and Palma ratio, measures of income inequality, our statistics are robust measures of a network's invariance to perturbations that accurately predict generalization gaps, i.e., the difference between accuracy on training and test sets.	Brian Quanz	blquanz@us.ibm.com	Yair  Schiff (IBM ); Brian Quanz (IBM Research)*; Payel Das (IBM Research); Pin-Yu Chen (IBM Research)	Schiff, Yair ; Quanz, Brian*; Das, Payel; Chen, Pin-Yu	yair.schiff@ibm.com; blquanz@us.ibm.com*; daspa@us.ibm.com; pin-yu.chen@ibm.com			2	2	0	0	0	Disabled (0)	Accept	Yes	Yes	No	No	gi_and_pal_scores_ICLR_robustness_workshop_final.pdf (393,740 bytes)	1		0	Laurent Callot (Amazon Research); Xian Li (Facebook)	lcallot@amazon.com; xianl@fb.com	Di Jin (MIT)	jindi15@mit.edu			No
30	2/25/2021 2:51:01 PM -08:00	2/25/2021 2:51:01 PM -08:00	Towards Robustness to Label Noise in Text Classification via Noise Modeling	Large datasets in NLP suffer from noisy labels, due to erroneous automatic and human annotation procedures. We study the problem of text classification with label noise, and aim to capture this noise through an auxiliary noise model over the classifier. We first assign a probability score to each training sample of having a noisy label, through a beta mixture model fitted on the losses at an early epoch of training. Then, we use this score to selectively guide the learning of the noise model and classifier. Our empirical evaluation on two text classification tasks shows that our approach can improve over the baseline accuracy, and prevent over-fitting to the noise.	Siddhant Garg	sidgarg@amazon.com	Siddhant Garg (Amazon Alexa AI Search)*; Goutham Ramakrishnan (Health at Scale Corporation); Varun Thumbe (KLA Corporation)	Garg, Siddhant*; Ramakrishnan, Goutham; Thumbe, Varun	sidgarg@amazon.com*; goutham7r@gmail.com; thumbe@cs.wisc.edu			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR2021 - Text Label Noise.pdf (631,638 bytes)	1		0	Ananya Kumar (DeepMind); Peter Hase (University of North Carolina at Chapel Hill)	skywalker94@gmail.com; peter@cs.unc.edu	Di Jin (MIT)	jindi15@mit.edu			No
31	2/25/2021 3:17:49 PM -08:00	3/6/2021 1:37:05 AM -08:00	More Than Meets The Eye: Semi-supervised Learning Under Non-IID Data	A common heuristic in semi-supervised deep learning (SSDL) is to select unlabelled data based on a notion of semantic similarity to the labelled data. For example, labelled images of numbers should be paired with unlabelled images of numbers instead of, say, unlabelled images of cars. We refer to this practice as semantic data set matching. In this work, we demonstrate the limits of semantic data set matching. We show that it can sometimes even degrade the performance for a state of the art SSDL algorithm. We present and make available a comprehensive simulation sandbox, called non-IID-SSDL, for stress testing an SSDL algorithm under different degrees of distribution mismatch between the labelled and unlabelled data sets. In addition, we demonstrate that simple density based dissimilarity measures in the  feature space of a generic classifier offer a promising and more reliable quantitative matching criterion to select unlabelled data before SSDL training.	Luis Oala	luis.oala@hhi.fraunhofer.de	Saul C Calderon-Ramirez (Instituto Tecnologico de Costa Rica); Luis Oala (Fraunhofer Institute for Telecommunications - Heinrich Hertz Institute)*	Calderon-Ramirez, Saul C; Oala, Luis*	sacalderon@itcr.ac.cr; luis.oala@hhi.fraunhofer.de*			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	More_Than_Meets_The_Eye__Semi_supervised_Learning_Under_Non_IID_Data.pdf (368,547 bytes)	1		0	Taylan Cemgil (DeepMind); Zhenlin Xu (UNC Chapel Hill)	taylancemgil@google.com; zhenlinx@cs.unc.edu	Di Jin (MIT)	jindi15@mit.edu			
32	2/25/2021 9:29:30 PM -08:00	2/25/2021 9:29:30 PM -08:00	IMPROVING LOCAL EFFECTIVENESS FOR GLOBAL ROBUST TRAINING	Many successful robust training methods rely on strong adversaries, which can be prohibitively expensive to generate when the input dimension is high and the model structure is complicated. We adopt a new perspective on robustness and propose a novel training algorithm that allows a more effective use of adversaries. Our method improves the model robustness at each local ball centered around an adversary and then, by combining these local balls through a global term, achieves overall robustness. Focusing on local balls improves the efficiency of robust training. We demonstrate the performance of our method on MNIST, CIFAR-10, CIFAR-100.	Jingyue Lu	jingyue.lu@spc.ox.ac.uk	Jingyue Lu (University of Oxford)*; M. Pawan Kumar (University of Oxford)	Lu, Jingyue*; Kumar, M. Pawan	jingyue.lu@spc.ox.ac.uk*; pawan@robots.ox.ac.uk			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Robust_iclr_worshop.pdf (931,506 bytes)	1		0	Hadi Salman (MIT); Sailik Sengupta (Arizona State University)	hady@mit.edu; sailiks@asu.edu	Di Jin (MIT)	jindi15@mit.edu			
33	2/25/2021 11:06:10 PM -08:00	2/25/2021 11:06:10 PM -08:00	On Adversarial Robustness: A Neural Architecture Search perspective	Adversarial robustness of deep learning models has gained much traction in the last few years. While a lot of approaches have been proposed to improve adversarial robustness, one promising direction for improving adversarial robustness is un-explored, i.e., the complex topology of the neural network architecture. In this work, we empirically understand the effect of architecture on adversarial robustness by experimenting with different hand-crafted and NAS based architectures. Our findings show that, for small-scale attacks, NAS-based architectures are more robust for small-scale datasets and simple tasks than hand-crafted architectures. However, as the dataset's size or the task's complexity increase, hand-crafted architectures are more robust than NAS-based architectures. We perform the first large scale study to understand adversarial robustness purely from an \textit{architectural perspective}. Our results show that random sampling in the search space of DARTS (a popular NAS method) with simple ensembling can improve the robustness to PGD attack by nearly ~12\%. We show that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy as a \textit{free add-on} without any form of adversarial training.  We also introduce a metric that can be used to calculate the trade-off between clean accuracy and adversarial robustness. 	Chaitanya Devaguptapu	cs19mtech11025@iith.ac.in	Chaitanya Devaguptapu (Indian Institute of Technology, Hyderabad)*; Gaurav Mittal (Microsoft); Devansh Agarwal (IIT Hyderabad); Vineeth N Balasubramanian (Indian Institute of Technology, Hyderabad)	Devaguptapu, Chaitanya*; Mittal, Gaurav; Agarwal, Devansh; N Balasubramanian, Vineeth	cs19mtech11025@iith.ac.in*; Gaurav.Mittal@microsoft.com; thedevanshagarwal@gmail.com; vineethnb@iith.ac.in			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	Submission_ICLRW.pdf (509,173 bytes)	1		0	Krishnamurthy Dvijotham (DeepMind); Maksym Andriushchenko (EPFL)	dvij@google.com; maksym.andriushchenko@epfl.ch	Di Jin (MIT)	jindi15@mit.edu			
35	2/26/2021 3:14:06 AM -08:00	2/26/2021 3:14:06 AM -08:00	Steering VQ-VAE to Improve OOD Robustness on a Medical Imaging Task	Deep learning has made remarkable progress in solving tasks on large scale, labelled datasets. However, performance degrades when the model is expected to generalize to out of distribution (OOD) samples. This challenge is compounded when samples follow a long tail distribution where there are only a few samples under a given property. We explore using steerable directions as a controllable data augmentation technique to improve robustness to these challenges. In brief, we find directions in latent space that correspond to a desired change in image space (e.g.~manipulating the staining properties of a cell image). This allows the generation of diverse, semantically meaningful samples. We evaluate our approach on the Camelyon dataset where the task is to determine if cell images from different hospitals with different staining properties contain tumor cells. On this dataset, using our steerable directions improves baseline performance by 4% in the OOD case (the hospital is unseen at training time) and 19% in the low data setting (only a small number of samples from the given hospital are seen at training time).	Olivia Wiles	oawiles@google.com	Olivia Wiles (DeepMind)*; Florian Stimberg (); Dan Andrei Calian (DeepMind); Timothy  Arthur Mann (DeepMind); Sven Gowal (DeepMind)	Wiles, Olivia*; Stimberg, Florian; Calian, Dan Andrei; Mann, Timothy  Arthur; Gowal, Sven	oawiles@google.com*; stimberg@google.com; dancalian@google.com; timothymann@google.com; sgowal@google.com			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	submission.pdf (2,058,469 bytes)	1		0	Di Jin (MIT); Jeremiah Liu (Google Research)	jindi15@mit.edu; jereliu@google.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No.
36	2/26/2021 4:56:39 AM -08:00	3/30/2021 9:48:03 AM -07:00	Neural Lower Bounds for Verification	Recent years have witnessed the deployment of branch-and-bound (BaB) frameworks for formal verification in deep learning. The main computational bottleneck of BaB is the estimation of lower bounds. Past work in this field has relied on traditional optimization algorithms whose inefficiencies have limited their scope. To alleviate this deficiency, we propose a novel graph neural network (GNN) based approach. Our GNN architecture closely resembles the network we wish to verify. During inference, it performs forward-backward passes through the GNN layers to compute a dual solution of the convex relaxation, thereby providing a valid lower bound. During training, its parameters are estimated via a loss function that encourages large lower bounds over a time horizon. We show that our approach provides a significant speedup for formal verification compared to state-of-the-art solvers and achieves good generalization performance on unseen networks.	Florian Jaeckle	florian.jaeckle@gmail.com	Florian Jaeckle (University of Oxford)*; M. Pawan Kumar (University of Oxford)	Jaeckle, Florian*; Kumar, M. Pawan	florian.jaeckle@gmail.com*; pawan@robots.ox.ac.uk			2	2	2	100	0	Disabled (0)	Accept (oral)	Yes	Yes	No	No	RobustML_workshop_ICLR2021_(2).pdf (377,911 bytes)	1		0	Dan Andrei Calian (DeepMind); Minhao Cheng (UCLA)	dancalian@google.com; mhcheng@g.ucla.edu	Di Jin (MIT)	jindi15@mit.edu			
38	2/26/2021 6:57:07 AM -08:00	4/13/2021 4:12:11 AM -07:00	Delta-CLUE: Diverse Sets of Explanations for Uncertainty Estimates	To interpret uncertainty estimates from differentiable probabilistic models, recent work has proposed generating Counterfactual Latent Uncertainty Explanations (CLUEs). However, for a single input, such approaches could output a variety of explanations due to the lack of constraints placed on the explanation. Here we augment the original CLUE approach, to provide what we call $\delta$-CLUE. CLUE indicates \emph{one} way to change an input, while remaining on the data manifold, such that the model becomes more confident about its prediction. We instead return a \emph{set} of plausible CLUEs: multiple, diverse inputs that are within a $\delta$ ball of the original input in latent space, all yielding confident predictions.	Dan Ley	dwl36@cam.ac.uk	Dan Ley (University of Cambridge)*; Umang Bhatt (University of Cambridge); Adrian Weller (University of Cambridge)	Ley, Dan*; Bhatt, Umang; Weller, Adrian	dwl36@cam.ac.uk*; usb20@cam.ac.uk; aw665@cam.ac.uk			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICLRW_2021__Delta_CLUE.pdf (4,154,119 bytes)	1		0	Pin-Yu Chen (IBM Research); Zhenlin Xu (UNC Chapel Hill)	pin-yu.chen@ibm.com; zhenlinx@cs.unc.edu	Di Jin (MIT)	jindi15@mit.edu			No
39	2/26/2021 8:25:57 AM -08:00	2/26/2021 8:47:08 AM -08:00	Failing Conceptually: Concept-Based Explanations of Dataset Shift	Despite their remarkable performance on a wide range of visual tasks, machine learning technologies often succumb to data distribution shifts. Consequently, a range of recent work explores techniques for detecting these shifts. Unfortunately, current techniques offer no explanations about what triggers the detection of shifts, thus limiting their utility to provide actionable insights. In this work, we present Concept Bottleneck Shift Detection (CBSD): a novel explainable shift detection method. CBSD provides explanations by identifying and ranking the degree to which high-level human-understandable concepts are affected by shifts. Using two case studies (dSprites and 3dshapes), we demonstrate how CBSD can accurately detect underlying concepts that are affected by shifts and achieve higher detection accuracy compared to state-of-the-art shift detection methods.	Maleakhi A Wijaya	maw219@cam.ac.uk	Maleakhi A Wijaya (University of Cambridge)*; Dmitry Kazhdan (University of Cambridge); Botty Dimanov (University of Cambridge); Mateja Jamnik (University of Cambridge)	Wijaya, Maleakhi A*; Kazhdan, Dmitry; Dimanov, Botty; Jamnik, Mateja	maw219@cam.ac.uk*; dk525@cam.ac.uk; botty.dimanov@cl.cam.ac.uk; mateja.jamnik@cl.cam.ac.uk			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Concept_Bottleneck_Shift_Detection___ICLR_2021.pdf (2,678,194 bytes)	1		0	Di Jin (MIT); Jwala Dhamala (Amazon Alexa AI)	jindi15@mit.edu; jwaladhamala@gmail.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No, it has not.
40	2/26/2021 8:41:05 AM -08:00	3/2/2021 6:42:56 AM -08:00	A neural anisotropic view of underspecification in deep learning	The underspecification of most machine learning pipelines means that we cannot rely solely on validation performance to assess the robustness of deep learning systems to naturally occurring distribution shifts. Instead, making sure that a neural network can generalize across a large number of different situations requires to understand the specific way in which it solves a task. In this work, we propose to study this problem from a geometric perspective with the aim to understand two key characteristics of neural network solutions in underspecified settings: how is the geometry of the learned function related to the data representation? And, are deep networks always biased towards simpler solutions, as conjectured in recent literature?  We show that the way neural networks handle the underspecification of these problems is highly dependent on the data representation, affecting both the geometry and the complexity of the learned predictors. Our results highlight that understanding the architectural inductive bias in deep learning is fundamental to address the fairness, robustness, and generalization of these systems.	Guillermo Ortiz-Jimenez	guillermo.ortizjimenez@epfl.ch	Guillermo Ortiz-Jimenez (EPFL)*; Itamar F Salazar-Reque (Instituto Nacional de Investigación y Capacitación de Telecomunicaciones de la Universidad Nacional de Ingeniería); Apostolos Modas (EPFL); Seyed-Mohsen Moosavi-Dezfooli (ETH Zurich); Pascal Frossard (EPFL)	Ortiz-Jimenez, Guillermo*; Salazar-Reque, Itamar F; Modas, Apostolos; Moosavi-Dezfooli, Seyed-Mohsen; Frossard, Pascal	guillermo.ortizjimenez@epfl.ch*; itamarf.salazar@gmail.com; apostolos.modas@epfl.ch; seyed.moosavi@inf.ethz.ch; pascal.frossard@epfl.ch			4	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	NADs_underspecification__ICLRw_(1).pdf (6,560,323 bytes)	1		0	Shibani Santurkar (MIT); Yicheng Wang (University of North Carolina at Chapel Hill)	shibani@mit.edu; yeech.wang@gmail.com	Di Jin (MIT)	jindi15@mit.edu			
41	2/26/2021 1:30:21 PM -08:00	3/6/2021 3:29:54 AM -08:00	Spliced Binned-Pareto Distribution for robust modeling of heavy-tailed time series	This work proposes a novel method to robustly and accurately model time series with heavy-tailed noise, in non-stationary scenarios. In many practical application time series have heavy-tailed noise that significantly impacts the performance of classical forecasting models; in particular, accurately modeling a distribution over extreme events is crucial to performing accurate time series anomaly detection. We propose a Spliced Binned-Pareto distribution which is both robust to extreme observations and allows accurate modeling of the full distribution. Our method allows to capture time dependencies in the higher order moments of the distribu- tion such as the tail heaviness. We compare the robustness and the accuracy of the tail estimation of our method to other state of the art methods on Twitter mentions count time series.	Elena Ehrlich	ehrlichelena@gmail.com	Elena Ehrlich (Amazon Web Services)*; Laurent Callot (Amazon Research); François-Xavier Aubet (Amazon Research)	Ehrlich, Elena*; Callot, Laurent; Aubet, François-Xavier	ehrlichelena@gmail.com*; lcallot@amazon.com; aubetf@amazon.com			3	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	main.pdf (375,035 bytes)	1		0	WO JAE LEE (Purdue University); Yicheng Wang (University of North Carolina at Chapel Hill)	lee2465@purdue.edu; yeech.wang@gmail.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			This submission has not been submitted to an official conference proceeding.
44	2/26/2021 6:28:11 PM -08:00	2/26/2021 6:28:11 PM -08:00	An Investigation of the (In)effectiveness of Counterfactually Augmented Data	Numerous recent works show that current models exploit spurious correlations in benchmark datasets. Even though they achieve excellent in-distribution performance, they generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD)---data generated by minimally perturbing examples to flip the ground-truth label---to identify robust features. However, the OOD generalization results using CAD have been mixed (e.g. on natural language inference and question answering). To understand CAD better and explain this discrepancy, we draw insights from a simple linear regression model and demonstrate the potential pitfalls of CAD and how they are evident in current datasets. Specifically, we show that perturbations corresponding to one robust feature may not be useful for learning other robust features, and can hurt performance in worst-case scenarios. Our results suggest that CAD is limited by the specific robust features which are perturbed during crowdsourcing, and it is not immediately consequent that they would help OOD generalization.	Nitish Joshi	joshinh@gmail.com	Nitish Joshi (New York University)*; He He (NYU)	Joshi, Nitish*; He, He	joshinh@gmail.com*; hehe@cs.nyu.edu			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR_Workshop___Counterfactual_Data.pdf (217,363 bytes)	1		0	Li Erran Li (Amazon / Columbia University); Pang Wei Koh (Stanford University)	erranlli@gmail.com; pangwei@cs.stanford.edu	Di Jin (MIT)	jindi15@mit.edu			No
45	2/26/2021 6:36:28 PM -08:00	3/6/2021 2:55:40 AM -08:00	Understanding Out-of-Distribution Detection with Deep Generative Models	Deep generative models (DGMs) seem a natural fit for out-of-distribution (OOD) detection. However, they have been shown assign higher probabilities to OOD images than in-distribution ones. Recent works try to explain this phenomenon as a problem with considering only low probability or density points as OOD, contending that even samples in high probability or density regions should be OOD in certain situations. In this work, we present the consequences of  broadening the definition of OOD: the task becomes impossible, and even a perfect model can perform worse than a misestimated one. We consider instead that the observed phenomenon is evidence that current DGMs are not yet good enough for OOD detection of certain out-distributions. We also suggest that improving deep generative models for out-of-distribution detection may require considerations other than those used to enable high likelihoods or good sample quality.	Lily Zhang	lily.h.zhang@nyu.edu	Lily Zhang (New York University)*; Mark Goldstein (New York University); Rajesh Ranganath (New York University)	Zhang, Lily*; Goldstein, Mark; Ranganath, Rajesh	lily.h.zhang@nyu.edu*; goldstein@nyu.edu; rajeshr@cims.nyu.edu			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	ood_iclr1 (2).pdf (1,030,883 bytes)	1		0	Mingjie Sun (Carnegie Mellon University); Yicheng Wang (University of North Carolina at Chapel Hill)	sunmj15@gmail.com; yeech.wang@gmail.com	Di Jin (MIT)	jindi15@mit.edu			
46	2/26/2021 6:44:43 PM -08:00	3/6/2021 4:00:35 AM -08:00	Towards Simple Yet Effective Transferable Targeted Adversarial Attacks	Transfer-based targeted adversarial attacks against deep image classifiers remain an open issue. Depending on which parts of the deep neural network are explicitly incorporated into the loss function, the existing methods can be divided into two categories: (a) feature space attack and (b) output space attack. One recent work has shown that attacking the feature space outperforms attacking the output space by a large margin. However, the elevated attack success comes at the cost of requiring to train layer-wise auxiliary classifiers for each corresponding target class together with a greedy search to find the optimal layers. In this work, we revisit the output space attack and improve it from two perspectives: First, we identify over-fitting as one major factor that hinders transferability, for which we propose to augment the network input and/or feature layers with noise. Second, we propose a new cross-entropy loss with two ends: one for pushing the sample far from the source class, i.e. ground-truth class, and the other for pulling it close to the target class. We find that given sufficiently large iterations, our approach can outperform the state-of-the-art feature space method by a large margin. 	Philipp Benz	pbenz@kaist.ac.kr	Philipp Benz (KAIST)*; Chaoning Zhang (KAIST); Adil Karjauv (KAIST); In So Kweon (KAIST)	Benz, Philipp*; Zhang, Chaoning; Karjauv, Adil; Kweon, In So	pbenz@kaist.ac.kr*; chaoningzhang1990@gmail.com; mikolez@gmail.com; iskweon77@kaist.ac.kr			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICLR_2021_Workshop__Improving_Transferability_of_Targeted_Adversarial_Attack (1).pdf (198,403 bytes)	1		0	Huan Zhang (UCLA); Maksym Andriushchenko (EPFL)	huanzhang@ucla.edu; maksym.andriushchenko@epfl.ch	Di Jin (MIT)	jindi15@mit.edu			No, it will probably be submitted to ICCV2021.
47	2/27/2021 2:58:36 AM -08:00	2/27/2021 3:19:03 AM -08:00	Is Disentanglement all you need? Comparing Concept-based & Disentanglement Approaches	Concept-based explanations have emerged as a popular way of extracting human-interpretable representations from deep discriminative models. At the same time, the disentanglement learning literature has focused on extracting similar representations in an unsupervised or weakly-supervised way, using deep generative models. Despite the overlapping goals and potential synergies, to our knowledge, there has not yet been a systematic comparison of the limitations and trade-offs between concept-based explanations and disentanglement approaches. In this paper, we give an overview of these fields, comparing and contrasting their properties and behaviours on a diverse set of tasks, and highlighting their potential strengths and limitations. In particular, we demonstrate that state-of-the-art approaches from both classes can be data inefficient, sensitive to the specific nature of the classification/regression task, or sensitive to the employed concept representation.	Dmitry Kazhdan	dk525@cam.ac.uk	Dmitry Kazhdan (University of Cambridge)*; Botty Dimanov (University of Cambridge); Helena Andres Terre (University of Cambridge); Pietro Lió (University of Cambridge); Mateja Jamnik (University of Cambridge); Adrian Weller (University of Cambridge)	Kazhdan, Dmitry*; Dimanov, Botty; Andres Terre, Helena; Lió, Pietro; Jamnik, Mateja; Weller, Adrian	dk525@cam.ac.uk*; botty.dimanov@cl.cam.ac.uk; ha376@cam.ac.uk; pl219@cam.ac.uk; mateja.jamnik@cl.cam.ac.uk; aw665@cam.ac.uk			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Semi_Supervised_Concept_Labelling (2).pdf (921,278 bytes)	1		0	Payel Das (IBM Research); Sailik Sengupta (Arizona State University)	daspa@us.ibm.com; sailiks@asu.edu	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No.
48	2/27/2021 3:43:04 AM -08:00	2/27/2021 3:43:04 AM -08:00	Hierarchical Variational Auto-Encoding for Unsupervised Domain Generalization	We address the task of domain generalization, where the goal is to train a predictive model such that it is able to generalize to a new, previously unseen domain. We choose a generative approach within the framework of variational autoencoders and propose an unsupervised algorithm that is able to generalize to new domains without supervision. We show that our method is able to learn representations that disentangle domain-specific information from class-label specific information even in complex settings where domain structure is not observed during training. Our interpretable method outperforms previously proposed generative algorithms for domain generalization and achieves competitive performance compared to state-of-the-art approaches, which rely on observing domain-specific information during training, on the standard domain generalization benchmark dataset PACS. Additionally, we proposed weak domain supervision which can further increase the performance of our algorithm in the PACS dataset.	Florian Buettner	fbuettner.phys@gmail.com	Xudong Sun (LMU Munich); Florian Buettner (Siemens AG)*	Sun, Xudong; Buettner, Florian*	smilesun.east@gmail.com; fbuettner.phys@gmail.com*			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	domain_generalization_hdiva (5).pdf (395,870 bytes)	1		0	Julius Adebayo (MIT); Peter Hase (University of North Carolina at Chapel Hill)	juliusad@mit.edu; peter@cs.unc.edu	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			
49	2/27/2021 3:44:02 AM -08:00	2/27/2021 3:44:02 AM -08:00	Asymmetry and Heavy Tails: Built-In Robustness in Classification	Real-world machine learning applications often require making seemingly innocuous choices, such as picking a classification loss function. Our objective is to show that robustness should already be a consideration at that stage. This paper proposes a new analysis of robustness in this set-up via a probabilistic analysis of margin maximisation. In particular, asymmetry and tail behaviour are important components that determine the robustness of usual functions such as exponential loss, binomial deviance, or hinge loss. We show that each loss function can be linked to an underlying distribution with different qualitative properties.	Francois Buet-Golfouse	francois.buetgolfouse@gmail.com	Francois Buet-Golfouse (UCL)*	Buet-Golfouse, Francois*	francois.buetgolfouse@gmail.com*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR21_Robust_Losses (1).pdf (305,670 bytes)	1		0	Julius Adebayo (MIT); zhiting hu (Amazon / UC San Diego)	juliusad@mit.edu; zhitinghu@gmail.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No, this work has never been submitted.
50	2/27/2021 4:11:29 AM -08:00	4/20/2021 11:24:42 AM -07:00	Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks	We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of 3.4% errors across the 10 datasets, where for example 2916 label errors comprise 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (54% of the algorithmically-flagged candidates are indeed erroneously labeled). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet- 50 if the prevalence of originally mislabeled test examples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.	Curtis G Northcutt	curtis@chipbrain.com	Curtis G Northcutt (ChipBrain)*; Anish Athalye (Massachusetts Institute of Technology); Jonas Mueller (AWS)	Northcutt, Curtis G*; Athalye, Anish; Mueller, Jonas	curtis@chipbrain.com*; aathalye@mit.edu; jonasmue@amazon.com			0	2	2	100	0	Disabled (0)	Accept (oral)	Yes	Yes	No	No	Label_Errors_ICLR_2021__RobustML_Workshop_ (1).pdf (2,201,966 bytes)	1		0	Brett Beaulieu-Jones (Harvard Medical School); Jwala Dhamala (Amazon Alexa AI)	Brett_Beaulieu-Jones@hms.harvard.edu; jwaladhamala@gmail.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			An 8-page, full-manuscript version of this 4-page workshop submission was also submitted to ICML 2021 and to the ICLR 2021 WeaSuL workshop.
51	2/27/2021 4:14:43 AM -08:00	2/27/2021 4:16:48 AM -08:00	Where do models go wrong? Parameter-space saliency maps for explainability	Conventional approaches to explaining the decisions of neural networks for image data focus on the use of saliency maps, which highlight important inputs to which predictions are highly sensitive. However, these approaches largely ignore the inner workings of neural networks and the mechanisms responsible for particular inference outcomes.  In this work, we consider saliency methods which attend to the network parameters rather than inputs. These methods allow the user to explore how image features cause specific network components to malfunction and query the dataset for other images leading to similar erroneous behavior.	Roman Levin	rilevin@uw.edu	Roman Levin (University of Washington)*; Manli Shu (University of Maryland, College Park); Eitan Borgnia (University of Maryland); Furong Huang (University of Maryland); Micah Goldblum (University of Maryland); Tom Goldstein (University of Maryland, College Park)	Levin, Roman*; Shu, Manli; Borgnia, Eitan; Huang, Furong; Goldblum, Micah; Goldstein, Tom	rilevin@uw.edu*; manlis@umd.edu; eborgnia2@gmail.com; furongh@cs.umd.edu; goldblumcello@gmail.com; tomg@cs.umd.edu			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Parameter_Saliency_final.pdf (8,590,195 bytes)	1		0	Javid Ebrahimi (Visa); Pang Wei Koh (Stanford University)	jebivid@gmail.com; pangwei@cs.stanford.edu	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No
52	2/27/2021 6:42:23 AM -08:00	2/27/2021 6:42:23 AM -08:00	Robustness evaluation of a Convolutional Neural Network for the classification of single cells in Acute Myeloid Leukemia	Robustness to domain-characteristic perturbations is a key prerequisite for deploying neural networks in a real-world diagnostic setting for morphological classification of cytological images, thus closing the gap between proof of concept and routine use. Here, we present a comprehensive robustness analysis of a recently published network for the classification of malignant and non-malignant blood cells relevant for the diagnosis of Acute Myeloid Leukemia (AML) with respect to three modes of plausible image corruptions. We find that the network is robust to defocus blurring and JPEG compression, whereas performance deteriorates for changes in brightness. We show that retraining of the network with a corresponding augmentation strategy can ensure robustness also for brightness variation. Our analysis and training strategy paves the way for the application of neural networks in clinical and laboratory settings for rapid, reproducible and reliable single-cell classification.	Christian Matek	christian.matek@helmholtz-muenchen.de	Christian Matek (Helmholtz Zentrum Munich)*; Carsten Marr (Helmholtz-muenchen)	Matek, Christian*; Marr, Carsten	christian.matek@helmholtz-muenchen.de*; Carsten.marr@helmholtz-muenchen.de			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	iclr2021_conference.pdf (1,210,013 bytes)	1		0	aashwin mishra (Stanford University); Javid Ebrahimi (Visa)	aashwin@stanford.edu; jebivid@gmail.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			
53	2/28/2021 1:14:57 PM -08:00	2/28/2021 1:14:57 PM -08:00	Analyzing when MaxEnt RL Solves Robust Reinforcement Learning	Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.	Ben Eysenbach	beysenba@cs.cmu.edu	Ben Eysenbach (Carnegie Mellon University)*; Sergey Levine (UC Berkeley)	Eysenbach, Ben*; Levine, Sergey	beysenba@cs.cmu.edu*; svlevine@eecs.berkeley.edu			0	2	0	0	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICML_2021__MaxEnt_and_Robust_RL (2).pdf (5,617,670 bytes)	1		0	Laurent Callot (Amazon Research); Mahyar Fazlyab (Johns Hopkins University)	lcallot@amazon.com; mahyarfazlyab@jhu.edu	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No. It is currently in submission to ICML 2021.
54	2/28/2021 7:06:19 PM -08:00	3/5/2021 10:29:55 PM -08:00	ATTACKS, DEFENSES, AND TOOLS: A FRAMEWORK TO FACILITATE ROBUST AI/ML SYSTEMS	Software systems are more increasingly relying on Artificial Intelligence (AI) and Machine Learning (ML) components. The emerging popularity of AI techniques in various application domains attracts malicious actors and adversaries. Therefore, the developers of AI-enabled software systems need to take into account various novel cyber-attacks and vulnerabilities that these systems may be susceptible to. This paper presents a framework to characterize attacks and weaknesses associated with AI-enabled systems and provide mitigation techniques and defense strategies. This framework aims to support software designers to take proactive measures in developing AI-enabled software, understanding the attack surface of such systems and develop products that are resilient to various emerging attacks associated with ML. The developed framework covers a broad spectrum of attacks, mitigation techniques, and defensive and offensive tools. In this paper, we demonstrate the framework architecture and its major components, describe their attributes, and discuss the long-term goals of this research.	Mohamad Fazelnia	mf8754@rit.edu	Mohamad Fazelnia (Rochester Institute of Technology)*; Igor Khokhlov (Rochester Institute of Technology); Mehdi Mirakhorli (Rochester Institute of Technology)	Fazelnia, Mohamad*; Khokhlov, Igor; Mirakhorli, Mehdi	mf8754@rit.edu*; ixk8996@rit.edu; mxmvse@rit.edu			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Attacks_to_AI_ML_knowledge_base.pdf (1,120,134 bytes)	1		0	Francesco Croce (University of Tübingen); Seyed-Mohsen Moosavi-Dezfooli (ETH Zurich)	francesco91.croce@gmail.com; seyed.moosavi@inf.ethz.ch	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			
56	3/3/2021 1:43:20 AM -08:00	3/3/2021 1:43:20 AM -08:00	Adversarial Training Blocks Generalization in Neural Policies	Deep neural networks have made it possible for reinforcement learning algorithms to learn from raw high dimensional inputs. This jump in the progress has caused deep reinforcement learning algorithms to be deployed in many different fields from financial markets to biomedical applications. While the vulnerability of deep neural networks to imperceptible specifically crafted perturbations has also been inherited by deep reinforcement learning agents, several adversarial training methods have been proposed to overcome this vulnerability. In this paper we focus on state-of-the-art adversarial training algorithms and investigate their robustness to semantically meaningful natural perturbations ranging from changes in brightness to rotation. We conduct several experiments in the OpenAI Atari environments, and find that state-of-the-art adversarially trained neural policies are more sensitive to natural perturbations than vanilla trained agents. We believe our investigation lays out intriguing properties of adversarial training and our observations can help build robust and generalizable neural policies.	Ezgi Korkmaz	ezgikorkmazk@gmail.com	Ezgi Korkmaz (KTH Royal Institute of Technology)*	Korkmaz, Ezgi*	ezgikorkmazk@gmail.com*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	advtrain.pdf (893,752 bytes)	1		0	Baoyuan Wu (The Chinese University of Hong Kong, Shenzhen); Leslie Rice (Carnegie Mellon University)	wubaoyuan@cuhk.edu.cn; larice@cs.cmu.edu	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			No.
57	3/3/2021 9:33:29 PM -08:00	3/3/2021 9:33:29 PM -08:00	Studying Classifier Robustness to Attribute-Level Shifts	While existing work in robust deep learning has focused on pixel-level $\ell_p$ norm-based perturbations, this class of perturbations does not account for many real-world deviations such as object-level shifts, geometric transformations, and weather-related artifacts. We consider a setup where robustness is expected over an unseen test domain that deviates from training domain in terms of attributes, specified \textit{a} priori. We propose an adversarial training approach which learns to generate new samples so as to maximize exposure of the classifier to the attributes-space. We introduce the CLEVR-Singles dataset that allows a controlled experimental setup to study robustness of classifiers to shifts along attributes.	Tejas Gokhale	tgokhale@asu.edu	Tejas Gokhale (Arizona State University)*; Rushil Anirudh (Lawrence Livermore National Laboratory); Bhavya Kailkhura (Lawrence Livermore National Laboratory); Jayaraman J. Thiagarajan (Lawrence Livermore National Laboratory); Chitta Baral (Arizona State University); Yezhou Yang (Arizona State University)	Gokhale, Tejas*; Anirudh, Rushil; Kailkhura, Bhavya; J. Thiagarajan, Jayaraman; Baral, Chitta; Yang, Yezhou	tgokhale@asu.edu*; anirudh1@llnl.gov; kailkhura1@llnl.gov; jjayaram@llnl.gov; chitta@asu.edu; yz.yang@asu.edu			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	iclr_robustml_agat.pdf (838,278 bytes)	1		0	Philipp Benz (KAIST); Sven Gowal (DeepMind)	pbenz@kaist.ac.kr; sgowal@google.com	Yixin Nie (UNC-Chapel Hill)	yixin1@cs.unc.edu			AAAI 2021
58	3/3/2021 11:33:43 PM -08:00	3/5/2021 11:17:18 PM -08:00	SI-Score: An image dataset for fine-grained analysis of robustness to object location, rotation and size	Before deploying machine learning models it is critical to assess their robustness. In the context of deep neural networks for image understanding, changing the object location, rotation and size may affect the predictions in non-trivial ways. In this work we perform a fine-grained analysis of robustness with respect to these factors of variation using SI-SCORE, a synthetic dataset. In particular, we investigate ResNets, Vision Transformers and CLIP, and identify interesting qualitative differences between these.	Jessica Yung	j.yung357@gmail.com	Jessica Yung (Google Research)*; Rob Romijnders (Google AI); Alexander Kolesnikov (Google Brain); Lucas Beyer (Google Brain); Josip Djolonga (Google AI, Zurich); Neil Houlsby (Google); Sylvain Gelly (Google Brain); Mario Lucic (Google Brain); Xiaohua Zhai (Google Brain)	Yung, Jessica*; Romijnders, Rob; Kolesnikov, Alexander; Beyer, Lucas; Djolonga, Josip; Houlsby, Neil; Gelly, Sylvain; Lucic, Mario; Zhai, Xiaohua	j.yung357@gmail.com*; romijndersrob@gmail.com; akolesnikov@google.com; lbeyer@google.com; josipd@google.com; neilhoulsby@google.com; sylvaingelly@google.com; lucic@google.com; xzhai@google.com			8	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	robustml-58.pdf (1,784,431 bytes)	1		0	Ammar Shaker (NEC Laboratories Europe); Kalpesh Krishna (UMass Amherst)	ammar.shaker@neclab.eu; kalpesh@cs.umass.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			This submission is an extension of a paper accepted to CVPR 2021.
59	3/4/2021 12:42:07 AM -08:00	3/4/2021 12:42:07 AM -08:00	Data Profiling for Adversarial Training	Multiple intriguing problems hover in adversarial training, including robustness-accuracy trade-off, robust overfitting, and gradient masking, posing great challenges to both reliable evaluation and practical deployment.  Here, we show that these problems share one common cause---low quality samples in the dataset.  We first identify an intrinsic property of the data called \emph{problematic score} and then design controlled experiments to investigate its connections with these problems. Specifically, we find that when problematic data is removed, robust overfitting and gradient masking can be largely alleviated; and robustness-accuracy trade-off is more prominent for a dataset containing highly problematic data. These observations not only verify our intuition about data quality but also open new opportunities to advance adversarial training.  Remarkably, simply removing problematic data from adversarial training, while making the training set smaller, yields better robustness consistently with different adversary settings, training methods, and neural architectures.	Chengyu Dong	cdong@ucsd.edu	Chengyu Dong (University of California, San Diego)*; Liyuan Liu (University of Illinois at Urbana Champaign); Jingbo Shang (UC San Diego)	Dong, Chengyu*; Liu, Liyuan; Shang, Jingbo	cdong@ucsd.edu*; llychinalz@gmail.com; jshang@eng.ucsd.edu			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	RobustML_dataProfiling.pdf (1,567,896 bytes)	1		0	Elan Rosenfeld (Carnegie Mellon University); Kalpesh Krishna (UMass Amherst)	ekr@andrew.cmu.edu; kalpesh@cs.umass.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No. But this submission is being reviewed by ICML 2021.
60	3/4/2021 3:02:17 AM -08:00	3/4/2021 3:02:17 AM -08:00	Adversariallary Trained Neural Policies in The Fourier Domain	Reinforcement learning policies based on deep neural networks are vulnerable to imperceptible adversarial perturbations to their inputs, in much the same way as neural network image classifiers. Recent work has proposed several methods for adversarial training for deep reinforcement learning agents to improve robustness to adversarial perturbations. In this paper, we study the effects of adversarial training on the neural policy learned by the agent. In particular, we compare the Fourier spectrum of minimal perturbations computed for both adversarially trained and vanilla trained neural policies. Via experiments in the OpenAI Atari environments we show that minimal perturbations computed for adversarially trained policies are more focused on lower frequencies in the Fourier domain, indicating a higher sensitivity of these policies to low frequency perturbations. We believe our results can be an initial step towards understanding the relationship between adversarial training and different notions of robustness for neural policies.	Ezgi Korkmaz	ezgikorkmazk@gmail.com	Ezgi Korkmaz (KTH Royal Institute of Technology)*	Korkmaz, Ezgi*	ezgikorkmazk@gmail.com*			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	advtrainfrequen.pdf (2,380,488 bytes)	1		0	Dimitris Tsipras (MIT); Pratyush Maini (IIT Delhi)	tsipras@mit.edu; pratyush.maini@gmail.com	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No
61	3/4/2021 8:58:41 AM -08:00	3/4/2021 8:58:41 AM -08:00	Learning interpretable latent dynamics for a 2D airfoil system	We develop a model, that is flexible enough to capture the complex unsteady aerodynamics of a 2D airfoil undergoing dynamic stall, but simple enough to be trained end-to-end with experimental data, to provide analytically tractable dynamics, and to propagate uncertainty. Provided with a specific motion trajectory, it outputs mean and uncertainty estimates for the pressure field and forces that act on the airfoil. To this end, the model learns a low-dimensional latent space, where the system dynamics are linear but parametrised by control variables, namely the angle of attack. Learning additionally a non-linear mapping from this latent space to the observed parameters, we show that the model provides qualitatively and physically correct predictions for trajectories of lift and pressure field outside of the training distribution.	Christian  Donner	christian.donner@sdsc.ethz.ch	Christian  Donner (Swiss Data Science Center)*; Natasa Tagasovska (EPFL); Guosheng He (EPFL); Karen Mulleners (EPFL); Hideaki Shimazaki (Hokkaido University); Guillaume Obozinski (Swiss Data Science Center)	Donner, Christian *; Tagasovska, Natasa; He, Guosheng; Mulleners, Karen; Shimazaki, Hideaki; Obozinski, Guillaume	christian.donner@sdsc.ethz.ch*; natasa.tagasovska@epfl.ch; guosheng.he@epfl.ch; karen.mulleners@epfl.ch; hideaki.shimazaki@gmail.com; guillaume.obozinski@epfl.ch			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	RobustML_ICLR21.pdf (999,514 bytes)	1		0	Kalpesh Krishna (UMass Amherst); Robin Jia (Stanford University)	kalpesh@cs.umass.edu; robinjia@stanford.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			
62	3/4/2021 10:18:16 AM -08:00	3/6/2021 2:34:24 AM -08:00	Post-Hoc Domain Adaptation via Guided Data Homogenization	Addressing shifts in data distributions is an important prerequisite for the deployment of deep learning models to real-world settings. A general approach to this problem involves the adjustment of models to a new domain through transfer learning. However, in many cases, this is not applicable in a post-hoc manner to deployed models and further parameter adjustments jeopardize safety certifications that were established beforehand. In such a context, we propose to deal with changes in the data distribution via guided data homogenization which shifts the burden of adaptation from the model to the data. This approach makes use of information about the training data contained implicitly in the deep learning model to learn a domain transfer function. This allows for a targeted deployment of models to unknown scenarios without changing the model itself. We demonstrate the potential of data homogenization through experiments on the CIFAR-10 and MNIST data sets. 	Kurt Willis	kurt.willis@hhi.fraunhofer.de	Kurt Willis (Fraunhofer Institute for Telecommunications - Heinrich Hertz Institute)*; Luis Oala (Fraunhofer Institute for Telecommunications - Heinrich Hertz Institute)	Willis, Kurt*; Oala, Luis	kurt.willis@hhi.fraunhofer.de*; luis.oala@hhi.fraunhofer.de			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Post_hoc_Guided_Data_Homogenization.pdf (3,035,907 bytes)	1		0	Pin-Yu Chen (IBM Research); Yang Li (Tsinghua-Berkeley Shenzhen Institute, Tsinghua University)	pin-yu.chen@ibm.com; yangli@sz.tsinghua.edu.cn	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			
65	3/4/2021 6:08:08 PM -08:00	3/5/2021 9:52:48 PM -08:00	Robust Audio Anomaly Detection	We propose an outlier robust multivariate time series model which can be used for detecting previously unseen anomalous sounds based on noisy training data. The presented approach doesn't assume the presence of labeled anomalies in the training dataset and uses a novel deep neural network architecture to learn the temporal dynamics of the multivariate time series at multiple resolutions while being robust to contaminations in the training dataset. The temporal dynamics are modeled using recurrent layers augmented with attention mechanism. These recurrent layers are built on top of convolutional layers allowing the network to extract features at multiple resolutions. The output of the network is an outlier robust probability density function modeling the conditional probability of future samples given the time series history. State-of-the-art approaches using other multiresolution architectures are contrasted with our proposed approach. We validate our solution using publicly available machine sound datasets. We demonstrate the effectiveness of our approach in anomaly detection by comparing against several state-of-the-art models.	Karim Helwani	karim.helwani@ieee.org	WO JAE LEE (Purdue University); Karim Helwani (Amazon)*; Arvindh Krishnaswamy (Amazon); Srikanth Tenneti (Amazon)	LEE, WO JAE; Helwani, Karim*; Krishnaswamy, Arvindh; Tenneti, Srikanth	lee2465@purdue.edu; karim.helwani@ieee.org*; arvindhk@amazon.com; stenneti@amazon.com			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR_2021_Workshop_main paper and supplementary materials4.pdf (3,102,735 bytes)	1		0	Rohan Taori (Stanford University); Xiang Zhou (UNC Chapel Hill)	rtaori@stanford.edu; xzh@cs.unc.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			
66	3/5/2021 12:44:53 AM -08:00	3/5/2021 12:44:53 AM -08:00	Inaccuracy of State-Action Value Function For Non-Optimal Actions in Adversarially Trained Deep Neural Policies	The introduction of deep neural networks as function approximator for the state-action value function has led to the creation of a new research area for self-learning systems that explore policies from high dimensional input. While the success of deep neural policies has resulted in the deployment of these policies in diversified application domains, there are significant concerns regarding their robustness towards specifically crafted malicious perturbations introduced to their inputs. Several studies have focused on making deep neural policies resistant to such perturbations via training with the existence of these perturbations (i.e. adversarial training). In this paper we focus on conducting an investigation on the state-action value function learned by state-of-the-art adversarially trained deep neural policies and vanilla trained deep neural policies. We perform several experiments in the OpenAI Baselines and we show that the state-action value functions learned by vanilla trained deep neural policies have better estimates for the non-optimal actions than the state-of-the-art adversarially trained deep neural policies. We believe our study lays out intriguing properties of adversarial training and could be critical step towards obtaining robust and reliable policies.	Ezgi Korkmaz	ezgikorkmazk@gmail.com	Ezgi Korkmaz (KTH Royal Institute of Technology)*	Korkmaz, Ezgi*	ezgikorkmazk@gmail.com*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	robustness.pdf (375,490 bytes)	1		0	Huan Zhang (UCLA); Minhao Cheng (UCLA)	huanzhang@ucla.edu; mhcheng@g.ucla.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No
67	3/5/2021 1:12:10 AM -08:00	3/5/2021 1:16:02 AM -08:00	On the effectiveness of adversarial training against common corruptions	The literature on robustness towards common corruptions shows no consensus on whether adversarial training can improve the performance in this setting. First, we show that, when used with an appropriately selected perturbation radius, $\l_p$ adversarial training can serve as a strong baseline against common corruptions. Then we explain why adversarial training performs better than data augmentation with simple Gaussian noise which has been observed to be a meaningful baseline on common corruptions. Related to this, we identify the \textit{$\sigma$-overfitting} phenomenon when Gaussian augmentation overfits to a particular standard deviation used for training which has a significant detrimental effect on common corruption accuracy. We discuss how to alleviate this problem and then how to further enhance $\l_p$ adversarial training by introducing an \textit{efficient relaxation} of adversarial training with \textit{learned perceptual image patch similarity} as the distance metric. Through experiments on CIFAR-10 and ImageNet-100, we show that our approach does not only improve the $\l_p$ adversarial training baseline but also has cumulative gains with data augmentation methods such as AugMix, ANT, and SIN leading to state-of-the-art performance on common corruptions.	Maksym Andriushchenko	maksym.andriushchenko@epfl.ch	Klim Kireev (EPFL); Maksym Andriushchenko (EPFL)*; Nicolas Flammarion (EPFL)	Kireev, Klim; Andriushchenko, Maksym*; Flammarion, Nicolas	klim.kireev@epfl.ch; maksym.andriushchenko@epfl.ch*; nicolas.flammarion@epfl.ch			4	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	On the effectiveness of adversarial training against common corruptions (ICLR'21 workshop).pdf (584,029 bytes)	1		0	Dan Andrei Calian (DeepMind); Dimitris Tsipras (MIT)	dancalian@google.com; tsipras@mit.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No.
68	3/5/2021 3:49:52 AM -08:00	3/5/2021 3:49:52 AM -08:00	Multi-Source Domain Adaptation with von Neumann Conditional Divergence	The similarity of feature representations plays a pivotal role in the success of domain adaptation and generalization. Feature similarity includes both the invariance of marginal distributions and the closeness of conditional distributions given the desired response $y$ (e.g., class labels). In this work, we introduce the recently proposed von Neumann conditional divergence to improve the transferability across multiple domains. We show that this new divergence is differentiable and eligible to easily quantify the functional dependence between features and $y$. We investigate the utility of this divergence in the problem of multi-source domain adaptation (for regression). A new learning objective and generalization bound are developed accordingly. We obtain favorable performance against state-of-the-art methods in terms of smaller generalization error on new tasks.	Ammar Shaker	ammar.shaker@neclab.eu	Ammar Shaker (NEC Laboratories Europe)*; Shujian Yu (NEC Laboratories Europe)	Shaker, Ammar*; Yu, Shujian	ammar.shaker@neclab.eu*; Shujian.Yu@neclab.eu			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR21_Workshop__Learning_to_Transfer_with_von_Neumann__Conditional__Divergence.pdf (668,397 bytes)	1		0	Robin Jia (Stanford University); Xiang Zhou (UNC Chapel Hill)	robinjia@stanford.edu; xzh@cs.unc.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			
70	3/5/2021 11:47:19 AM -08:00	3/5/2021 11:47:19 AM -08:00	Out Of Distribution Robustness and Uncertainty Quantification for Particle Accelerator Applications	Particle accelerators have found applications in diverse tasks such as cancer therapy, nuclear non-proliferation treaty verification, scientific discovery, etc. Such accelerators represent one of the most complex scientific devices, and are also extremely challenging to maintain and operate. Surrogate models for accelerators can aid in planning, design and operation, for instance in model based tuning. To this end, deep learning models are being used for surrogate modeling as they can process large corpora of high-dimensional data, ranging from scalars, to beam images and spectra. However, for the deployment of these models in such a high-regret and safety-critical system, verifiable out of distribution (OOD) robustness and reliable estimates of the predictive uncertainty are essential. We evaluate Bayesian Neural Networks as a means to provide OOD robustness, in conjunction with calibrated estimates of predictive uncertainty for particle accelerator systems. We outline application case studies across different accelerator designs (storage rings, beam lines for free electron lasers, injector systems). The cases have diverse data volumes and formats, e.g. particle beam phase space images and scalar parameters. We find that Bayesian Neural Networks provide accurate predictions with reliable uncertainty estimates, while ensuring robustness to dataset shifts. 	aashwin mishra	aashwin@stanford.edu	aashwin mishra (Stanford University)*; Auralee Edelen (SLAC National Accelerator Laboratory); Christopher  Mayes (SLAC National Accelerator Laboratory)	mishra, aashwin*; Edelen, Auralee; Mayes, Christopher 	aashwin@stanford.edu*; edelen@slac.stanford.edu; cmayes@slac.stanford.edu			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	BNN_UQ_OOD_ICLR_Workshop.pdf (805,396 bytes)	1		0	Mingjie Sun (Carnegie Mellon University); Taylan Cemgil (DeepMind)	sunmj15@gmail.com; taylancemgil@google.com	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No
71	3/5/2021 12:46:58 PM -08:00	3/5/2021 3:44:41 PM -08:00	Improving Model-moderator Collaborative Predictions with Uncertainty Estimation	As the scale of conversations online grows, content providers have increasingly incorporated machine learning models into moderation systems, operating in collaboration with human content moderators to detect toxic content given limited human time and attention. These machine learning models are typically evaluated using metrics like accuracy or AUROC. However, such metrics fail to capture the performance of the combined moderator-model system. Here, we propose an accuracy metric, Oracle-Model Collaborative Accuracy (OCA), that describes the overall system performance under constraints on human review capacity.  Moreover, we present a challenging data benchmark, CoToMoD, for evaluating the performance of collaborative toxic comment moderation systems. Using this benchmark, we evaluate the performance of several models using OCA and other metrics: our results demonstrate the importance of metrics capturing the collaborative nature of the moderator-model system for this task, as well as the utility of uncertainty estimation in this domain.	Zi Lin	lzi@google.com	Zi Lin (Google)*; Ian D Kivlichan (Jigsaw); Jeremiah Liu (Google Research); Lucy Vasserman (Google)	Lin, Zi*; Kivlichan, Ian D; Liu, Jeremiah; Vasserman, Lucy	lzi@google.com*; kivlichan@google.com; jereliu@google.com; lucyvasserman@google.com			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Uncertainty_paper__ICLR_workshop_ (1).pdf (593,514 bytes)	1		0	Di Jin (MIT); Shiori Sagawa (Stanford University)	jindi15@mit.edu; ssagawa@cs.stanford.edu	Tristan Naumann (Microsoft Research Redmond, US)	tristan@microsoft.com			No.
73	3/5/2021 1:13:43 PM -08:00	3/5/2021 1:13:43 PM -08:00	Continual Invariant Risk Minimization	Empirical risk minimization can lead to poor generalization behaviour on unseen environments if the learned model does not capture invariant feature representations. Invariant risk minimization (IRM) is a recent proposal for discovering environment-invariant representations robust to spurious features. It was introduced by Arjovsky et al. (2019) and extended by Ahuja et al. (2020). The assumption of IRM is that all environments are available to the learning system at the same time. With this work, we generalize the concept of IRM to scenarios where environments are observed sequentially. We show that existing approaches, including those designed for continual learning, fail to identify the invariant features and models across sequentially presented environments. We extend IRM under a variational Bayesian and bilevel framework, creating a general approach to continual invariant risk minimization. We also describe a strategy to solve the optimization problems using a variant of the alternating direction method of multiplier (ADMM). We show empirically using multiple datasets and with multiple sequential environments that the proposed methods outperforms or is competitive with prior approaches.	Francesco Alesiani	Francesco.Alesiani@neclab.eu	Francesco Alesiani (NEC Laboratories Europe)*; Shujian Yu (NEC Laboratories Europe)	Alesiani, Francesco*; Yu, Shujian	Francesco.Alesiani@neclab.eu*; Shujian.Yu@neclab.eu			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Continual_Invariant_Risk_Minimization_workshop (7).pdf (516,064 bytes)	1		0	Kuan-Hao Huang (University of California, Los Angeles); Olivia Wiles (DeepMind)	khhuang@cs.ucla.edu; ow@robots.ox.ac.uk	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			
75	3/5/2021 2:39:14 PM -08:00	3/6/2021 2:47:43 AM -08:00	Robust Machine Learning with Matrix-based Renyi's α-order Mutual Information	We introduce a recently proposed matrix-based Renyi’s α-order mutual information (denoted I_α) to measure the dependence (or independence) between two random variables (or vectors). We demonstrate that I_α is automatically differentiable and more statistically powerful than prevalent dependence (or independence) measures in identifying independence and discovering complex dependence patterns. We show the impacts of I_α for the robustness of deep neural networks, and put forward two proposals with both theoretical and empirical justiﬁcations.	Shujian Yu	Shujian.Yu@neclab.eu	Shujian Yu (NEC Laboratories Europe)*; Xi Yu (University of Florida); Francesco Alesiani (NEC Laboratories Europe); Jose Principe (University of Florida)	Yu, Shujian*; Yu, Xi; Alesiani, Francesco; Principe, Jose	Shujian.Yu@neclab.eu*; yuxi@ufl.edu; Francesco.Alesiani@neclab.eu; principe@cnel.ufl.edu			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	robustness_matrix_entropy_ICLR_workshop_final.pdf (617,962 bytes)	1		0	Jan Hendrik Metzen (Bosch Center for Artificial Intelligence); Pin-Yu Chen (IBM Research)	JanHendrik.Metzen@de.bosch.com; pin-yu.chen@ibm.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			Partial of the results shown in this submission has been accepted in AAAI-21 and ICASSP-21.

76	3/5/2021 2:57:01 PM -08:00	3/6/2021 3:16:32 AM -08:00	Learning Robust Controllers Via Probalistic Model-Based Policy Search	Model-based Reinforcement Learning estimates the true environment through a world model in order to approximate the optimal policy. This family of algorithms usually benefits from better sample efficiency than their model-free counterparts.We investigate whether controllers learned in such a way are robust and able to generalize under small perturbations of the environment.  Our work is inspired by the PILCO algorithm, a method for probabilistic policy search. We show that enforcing a lower bound to the likelihood noise in the Gaussian Process dynamics model  regularizes  the  policy  updates  and  yields  more  robust  controllers.   We demonstrate the empirical benefits of our method in a simulation benchmark.	Valentin Charvet	v.charvet.1@research.gla.ac.uk	Valentin Charvet (University of Glasgow)*; Bjoern  Sand Jensen (University of Glasgow); Roderick Murray-Smith (University of Glasgow)	Charvet, Valentin*; Jensen, Bjoern  Sand; Murray-Smith, Roderick	v.charvet.1@research.gla.ac.uk*; bjorn.jensen@glasgow.ac.uk; Roderick.Murray-Smith@glasgow.ac.uk			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	draft_safe_model_based_policy_search.pdf (307,102 bytes)	1		0	Jieliang Luo (Autodesk Research); Shujian Yu (NEC Laboratories Europe)	rodger.luo@autodesk.com; Shujian.Yu@neclab.eu	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			
77	3/5/2021 3:45:10 PM -08:00	3/30/2021 4:25:01 PM -07:00	Trustworthy AI development: From ethics principles to verifiable statements	Rapid research and development progress in artificial intelligence (AI) over the past decade has enabled widespread deployment of AI products and services in a wide range of domains. While many applications have been beneficial, or at least harmless, we have also seen numerous cases of harm from misuse, accidents or negative systemic effects such as amplification and entrenchment of discrimination. In recognition of these risks, and in response to employee pressure and increasing government scrutiny, developers of AI technologies have been adopting ethical principles that emphasise fairness, accountability and transparency, and promise to minimise harms. However, such self-regulation has proven insufficient both to prevent harms and to engender user trust. To go further, we need concrete mechanisms by which users, governments, and competitors can check that developers of AI technologies are fulfilling their stated ethical principles. This requires two key ingredients: (1) A concretisation of abstract principles into verifiable procedures and measurements, and (2) well-resourced mechanisms to carry out such verification in practice. Here we summarise the findings from 59 experts from academia, industry and NGOs that highlight ten mechanisms for increasing the verifiability of statements about AI development.	Tegan Maharaj	tegan.jrm@gmail.com	Tegan Maharaj (MILA, Polytechnic Montreal)*; Gretchen Krueger (OpenAI); Miles  Brundage (OpenAI); Haydn Belfield (); Shahar  Avin (University of Cambridge); Jasmine Wang ()	Maharaj, Tegan*; Krueger, Gretchen; Brundage, Miles ; Belfield, Haydn; Avin, Shahar ; Wang, Jasmine	tegan.jrm@gmail.com*; gretchen@openai.com; miles@openai.com; hb492@cam.ac.uk; sa478@cam.ac.uk; jasminewang76@gmail.com			0	2	1	50	0	Disabled (0)	Accept	Yes	No	No	No	Trustworthy_AI___ICLR_Robust_ML_workshop.pdf (258,521 bytes)	1		0	aashwin mishra (Stanford University); He He (NYU)	aashwin@stanford.edu; hehe@cs.nyu.edu	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			
79	3/5/2021 4:29:24 PM -08:00	3/5/2021 5:01:54 PM -08:00	The broken shield - face recognition, adversarial attacks, and the dangers of offering imperfect tools	In this article we survey adversarial attack tools for face detection and recognition models (FDRMs) and present and analyze a case study of risks associated with FDRMs and adversarial tools. We develop a framework for user-centered design of adversarial tools and draw lessons on risk disclosure from U.S. product liability, consumer protection, and negligence law. Combining these, we develop a set of concrete recommendations to help FDRM adversarial tool makers decrease user harm through better communication around risk.	Daniel Pedraza	daniel.pedraza@me.com	Gretchen Greene (Assembly Fellow, Berkman Klein Center for Internet & Society at Harvard University); Thom Miano (Assembly Fellow, Berkman Klein Center for Internet & Society at Harvard University); Daniel Pedraza (Assembly Fellow, Berkman Klein Center for Internet & Society at Harvard University)*; Kira Hessekiel (Harvard Law School)	Greene, Gretchen; Miano, Thom; Pedraza, Daniel*; Hessekiel, Kira	gretchen@bakerthomaslaw.com; tmiano@rti.org; daniel.pedraza@me.com*; khessekiel@cyber.harvard.edu			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	equalAIs_iclr2021_conference_vF.pdf (105,594 bytes)	1		0	He He (NYU); Saeid Asgari Taghanaki (Autodesk AI Lab)	hehe@cs.nyu.edu; saeid.asgari.taghanaki@autodesk.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			
80	3/5/2021 5:12:29 PM -08:00	3/5/2021 5:12:29 PM -08:00	Reachability Analysis on Recurrent Neural Networks	Verification of neural network plays an essential role in the safety analysis of neural networks. However, most of the recent works focus on the feed-forward neural network. In contrast, verification of the Recurrent Neural Networks (RNNs) is still a relatively young research area. Possible reasons are the special structure components of RNNs, such as gates and loops in LSTM and GRU, causing complexity and strong nonlinearity of the neural networks. We here employ a global optimization based method, RNNgo, to calculate the reachability of RNNs. Under the assumption of Lipschitz continuity, this technique can verify the neural network without the knowledge of inner detail structure. We first prove the Lipschitz continuity of the recurrent layers and then demonstrate RNNgo’s capability and efficiency in handling a wide range of RNNs. Through the reachability analysis, RNNgo also tackles the maximum safe radius problem, computing the minimum distance to an adversarial example for a given input example, concreting ground-truth adversarial examples.	Chi Zhang	cz338@exeter.ac.uk	Chi Zhang (University of Exeter)*; Fu Wang (Guilin University of Electronic and Technology, University of Exeter); Peipei Xu (University of Liverpool); Wenjie Ruan (University of Exeter)	Zhang, Chi*; Wang, Fu; Xu, Peipei; Ruan, Wenjie	cz338@exeter.ac.uk*; fuu.wanng@gmail.com; peipei.xu@liverpool.ac.uk; w.ruan@exeter.ac.uk			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Reachability analysis on RNNs.pdf (537,939 bytes)	1		0	Bhavya Kailkhura (Lawrence Livermore National Laboratory); Sven Gowal (DeepMind)	kailkhura1@llnl.gov; sgowal@google.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			No
81	3/5/2021 6:55:58 PM -08:00	3/5/2021 7:30:48 PM -08:00	Reachability Analysis of Neural Feedback Loops	Although neural networks (NNs) are traditionally trained to learn an input-output relationship from a dataset (e.g., classification, regression), real world deployments of machine learning (ML) are often part of complex, closed-loop systems, or neural feedback loops. For instance, robots with learned policies take actions that modify the world state and influence future observations. Thus, formal analysis of how NN decisions will play out in closed-loop systems is a fundamental component of robust and reliable ML in the real world. We address this issue by developing an algorithm which computes forward reachable sets -- bounds on the set of states a system could reach in the future -- using convex optimization. The proposed framework accounts for several sources of uncertainty that are present in the real world, such as in the initial conditions, and from measurement and process noise. Numerical experiments show 10x reduction in conservatism (which enables verification of tighter safety properties) in 1/2 of the computation time compared to the state-of-the-art, and the ability to handle various sources of uncertainty is highlighted on a quadrotor model.	Michael Everett	mfe@mit.edu	Michael Everett (MIT)*; Golnaz  Habibi (MIT); Jonathan How (MIT)	Everett, Michael*; Habibi, Golnaz ; How, Jonathan	mfe@mit.edu*; golnaz.habibi@gmail.com; jhow@mit.edu			2	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICLR_21_WS__Closed_Loop_Reachability_Analysis (2).pdf (1,333,751 bytes)	1		0	Hadi Salman (MIT); He He (NYU)	hady@mit.edu; hehe@cs.nyu.edu	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			It was accepted to ICRA 2021 (which is not until June 2021)
82	3/5/2021 8:11:55 PM -08:00	3/5/2021 8:11:55 PM -08:00	On the Robustness of Goal Oriented Dialogue Systems to Real-world Noise	Goal oriented dialogue systems in real-word environments often encounter noisy data. In this work, we investigate how robust these systems are to noisy data. Specifically, our analysis considers intent classification (IC) and slot labeling (SL) models that form the basis of most dialogue systems. We collect a test-suite for six common phenomena found in live human-to-bot conversations (abbreviations, casing, misspellings, morphological variants, paraphrases, and synonyms) and show that these phenomena can degrade the IC/SL performance of state-of-the-art BERT based models. Through the use of synthetic data augmentation, we are improve IC/SL model's robustness to real-world noise by +11.5\% for IC and +17.3 points for SL on average across noise types. We make our suite of noisy test data public to enable further research into the robustness of dialog systems.	Jason P Krone	kronej@amazon.com	Jason P Krone (AWS)*; Sailik Sengupta (Arizona State University); Saab Mansour (AWS)	Krone, Jason P*; Sengupta, Sailik; Mansour, Saab	kronej@amazon.com*; sailiks@asu.edu; saabm@amazon.com			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICLR_RobustML_Ws____On_the_Robustness_of_Goal_Oriented_Dialogue_Systems.pdf (142,596 bytes)	1		0	Chris Clark (UW); Xian Li (Facebook)	csquared@cs.washington.edu; xianl@fb.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			
83	3/5/2021 8:25:04 PM -08:00	3/5/2021 8:45:46 PM -08:00	Tasting the cake: evaluating self-supervised generalization on out-of-distribution multimodal MRI data	Self-supervised learning has enabled significant improvements on natural image benchmarks. However, there is less work in the medical imaging domain in this area. The optimal models have not yet been determined among the various options. Moreover, little work has evaluated the current applicability limits of novel self-supervised methods. In this paper, we evaluate a range of current contrastive self-supervised methods on out-of-distribution generalization in order to evaluate their applicability to medical imaging. We show that self-supervised models are not as robust as expected based on their results in natural imaging benchmarks and can be outperformed by supervised learning with dropout. We also show that this behavior can be countered with extensive augmentation. Our results highlight the need for out-of-distribution generalization standards and benchmarks to adopt the self-supervised methods in the medical imaging community.	Alex Fedorov	eidos92@gmail.com	Alex Fedorov (Georgia Institute of Technology)*; Eloy Geenjaar (TReNDS center, TU Delft); Lei Wu (Georgia State University); Thomas P. DeRamus (Georgia State University); Vince Calhoun (TReNDS); Sergey Plis (Georgia State University)	Fedorov, Alex*; Geenjaar, Eloy; Wu, Lei; DeRamus, Thomas P.; Calhoun, Vince; Plis, Sergey	eidos92@gmail.com*; e.geenjaar@gsu.edu; lwu9@gsu.edu; tderamus@gsu.edu; vcalhoun@gsu.edu; s.m.plis@gmail.com			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	robust_ml_workshop_iclr2021_final.pdf (1,768,281 bytes)	1		0	Aakanksha Naik (); Jeremiah Liu (Google Research)	anaik@cs.cmu.edu; jereliu@google.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			N/A
84	3/5/2021 9:01:12 PM -08:00	3/6/2021 12:04:45 AM -08:00	Mixture of Robust Experts (MoRE): A Flexible Defense Against Multiple Perturbations	To tackle the susceptibility of deep neural networks to adversarial examples, the adversarial training has been proposed which provides a notion of security through an inner maximization problem presenting the first-order adversaries embedded within the outer minimization of the training loss. To generalize the adversarial robustness over different perturbation types, the adversarial training method has been augmented with the improved inner maximization presenting a union of multiple perturbations e.g., various $\ell_p$ norm-bounded perturbations. However, the improved inner maximization only enjoys limited flexibility in terms of the allowable perturbation types. In this work, through a gating mechanism, we ensemble a set of expert networks, each one either adversarially trained to deal with a particular perturbation type or normally trained for boosting accuracy on clean data. The gating module assigns weights dynamically to each expert to achieve superior accuracy under various data types e.g., adversarial examples, adverse weather perturbations, and clean input. In order to deal with the obfuscated gradients issue, the training of the gating module is conducted together with fine-tuning of the last fully connected layers of expert networks through adversarial training approach. Using extensive experiments, we show that our Mixture of Robust Experts (MoRE) approach enables a flexible integration of a broad range of robust experts with superior performance.  On CIFAR-10, when evaluating only on $\ell_p$ adversarial perturbations, we achieve up to {5.54\%, 1.97\%, and 11.79\% increase in $\ell_2 (\epsilon=1.0)$ adversarial, $\ell_\infty  (\epsilon=8/255)$ adversarial, and clean accuracy}, respectively; and when evaluating on both $\ell_p$ adversarial perturbations and adverse weather perturbations, we achieve up to {14.19\%, 9.82\%, 12.37\% and 16.92\% increase in $\ell_2 (\epsilon=1.0)$, $\ell_\infty (\epsilon=8/255)$, weather, and clean accuracy}, respectively.	Hao Cheng	cheng.hao@northeastern.edu	Hao Cheng (Northeastern University)*; Kaidi Xu (Northeastern University); Chenan Wang (Northeastern University); Xue Lin (Northeastern University); Bhavya Kailkhura (Lawrence Livermore National Laboratory); Ryan Goldhahn (Lawrence Livermore National Lab)	Cheng, Hao*; Xu, Kaidi; Wang, Chenan; Lin, Xue; Kailkhura, Bhavya; Goldhahn, Ryan	cheng.hao@northeastern.edu*; xu.kaid@northeastern.edu; wang.chena@northeastern.edu; xue.lin@northeastern.edu; kailkhura1@llnl.gov; goldhahn1@llnl.gov			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR2021_RobustML_MoRE .pdf (310,052 bytes)	1		0	Leslie Rice (Carnegie Mellon University); Pratyush Maini (IIT Delhi)	larice@cs.cmu.edu; pratyush.maini@gmail.com	Zhijing Jin (University of Hong Kong)	zhijing.jin@connect.hku.hk			No.
85	3/5/2021 9:55:43 PM -08:00	3/5/2021 9:55:43 PM -08:00	Anomalous behaviour in loss-gradient based interpretability methods	Loss-gradients are used to interpret the decision making process of deep learning models. In this work, we evaluate loss-gradient based attribution methods by occluding parts of the input and comparing the performance of the occluded input to the original input. We observe that the occluded input has better performance than the original across the test dataset under certain conditions. Similar behaviour is observed in sound and image recognition tasks. We explore different loss-gradient attribution methods, occlusion levels and replacement values to explain the phenomenon of performance improvement under occlusion.	Vinod Subramanian	v.subramanian@qmul.ac.uk	Vinod Subramanian (Queen Mary University of London)*; Siddharth Kumar Gururani (Georgia Institute of Technology); Emmanouil Benetos (Queen Mary University of London); Mark B. Sandler (Queen Mary University of London)	Subramanian, Vinod*; Gururani, Siddharth Kumar; Benetos, Emmanouil; Sandler, Mark B.	v.subramanian@qmul.ac.uk*; siddfisher@gmail.com; emmanouil.benetos@qmul.ac.uk; mark.sandler@qmul.ac.uk			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Anomalous_behaviour_in_loss_gradient_based_interpretability_methods.pdf (223,523 bytes)	1		0	Eric Wallace (U.C. Berkeley); Jieliang Luo (Autodesk Research)	ericwallace@berkeley.edu; rodger.luo@autodesk.com	Eric Wong (MIT)	wongeric@mit.edu			
86	3/5/2021 10:48:28 PM -08:00	3/5/2021 10:48:28 PM -08:00	Responsible content filtering on social media and its cost	People form judgments and make decisions based on the information they observe. A growing portion of that information is not only provided but algorithmically filtered by social media platforms. In this way, content filtering has the ability to influence users' perceptions and behaviors, from dining choices to voting preferences. In this work, we present a statistical framework for quantifying the effect of content filtering on user learning. We derive conditions under which a platform's impact on user learning is undetectable with given confidence. This formulation suggests a principled framework for studying how the content chosen by a platform to maximize a given objective function (e.g., revenue or clicks) affects its users. We use this framework to investigate the common assumption that responsible content filtering always comes at a high performance cost. We show that the cost of responsibility can be low, particularly when the selected content is sufficiently diverse and the environment is time-varying. In fact, under certain conditions, there is no cost to responsibility. 	Sarah H Cen	shcen@mit.edu	Sarah H Cen (Massachusetts Institute of Technology)*; Devavrat Shah (MIT)	Cen, Sarah H*; Shah, Devavrat	shcen@mit.edu*; devavrat@mit.edu			1	2	2	100	0	Disabled (0)	Withdrawn	Yes	No	No	No	iclr2021_conference.pdf (330,307 bytes)	1		0	Eric Wallace (U.C. Berkeley); Jeremiah Liu (Google Research)	ericwallace@berkeley.edu; jereliu@google.com	Eric Wong (MIT)	wongeric@mit.edu			No.
87	3/5/2021 11:33:47 PM -08:00	3/6/2021 4:12:26 AM -08:00	Improved and Efficient Text Adversarial Attacks using Target Information	There has been recently a growing interest in studying adversarial examples on natural language models in the black-box setting. These methods attack natural language classifiers by perturbing certain important words until the classifier label is changed. In order to find these important words, these methods rank all words by importance by querying the target model word by word for each input sentence, resulting in high query inefficiency. A new interesting approach was introduced that addresses this problem through interpretable learning to learn the word ranking instead of previous expensive search. The main advantage of using this approach is that it achieves comparable attack rates to the state-of-the-art methods, yet faster and with fewer queries, where fewer queries are desirable to avoid suspicion towards the attacking agent. Nonetheless, this approach sacrificed the useful information that could be leveraged from the target classifier for that sake of query efficiency. In this paper we study the effect of leveraging the target model outputs and data on both attack rates and average number of queries, and we show that both can be improved, with a limited overhead of additional queries.	Mahmoud Hossam	mhossam@monash.edu	Mahmoud Hossam (Monash University)*; Trung Le (Monash University); He Zhao (Monash University); Viet Huynh (Monash University); Dinh Phung (Monash University)	Hossam, Mahmoud*; Le, Trung; Zhao, He; Huynh, Viet; Phung, Dinh	mhossam@monash.edu*; trunglm@monash.edu; ethan.zhao@monash.edu; viet.huynh@monash.edu; dinh.phung@monash.edu			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	main.pdf (588,939 bytes)	1		0	Jan Hendrik Metzen (Bosch Center for Artificial Intelligence); Shiori Sagawa (Stanford University)	JanHendrik.Metzen@de.bosch.com; ssagawa@cs.stanford.edu	Eric Wong (MIT)	wongeric@mit.edu			
88	3/6/2021 12:24:14 AM -08:00	3/6/2021 12:32:36 AM -08:00	Post hoc Explanations may be Ineffective for Diagnosing Unexpected Spurious Correlation	Detecting whether an overparametrized deep network has learned a `spurious association' from the training data is challenging. Post hoc model explanations are an increasingly promising avenue for addressing this challenge; however, it remains unclear whether they are effective.  We investigate whether three classes of post hoc explanations--feature attribution, concept ranking, and training point ranking--can detect \emph{unknown} spurious correlation in a high-stakes medical task. Through control experiments, we assess the ability of these classes of explanations to reliably identify model reliance on spurious signals in the training set. We find that the post hoc explanation approaches tested are able to detect spurious associations \textit{only} when the spurious signal is known \textit{a priori}. Given that the space of all possible spurious signals that a model could rely on is large and often unknown, this finding suggests that these approaches may be ineffective for detecting spurious signals in practice.	Julius Adebayo	juliusad@mit.edu	Julius Adebayo (MIT)*; Michael Muelly (Stanford University); Hal Abelson (MIT); Been Kim (Google)	Adebayo, Julius*; Muelly, Michael; Abelson, Hal; Kim, Been	juliusad@mit.edu*; mmuelly@stanford.edu; halabelson@csail.mit.edu; beenkim@google.com			2	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	radiology_debugging_iclr_2020.pdf (1,613,619 bytes)	1		0	Ananya Kumar (DeepMind); Jessica Yung (Google Research)	skywalker94@gmail.com; j.yung357@gmail.com	Eric Wong (MIT)	wongeric@mit.edu			No it has not.
89	3/6/2021 12:27:24 AM -08:00	3/30/2021 9:49:17 AM -07:00	A Causal Lens for Controllable Text Generation	Controllable text generation is one of the most important yet challenging tasks in natural language processing (NLP). Prior methods relying on disentangled representations or language models suffer from limited controllability due to biases or spurious correlations in the data. To remedy this, we present a causal framework for controllable text generation. Our method allows control over one or more attributes such as sentiment and cuisine type and perform counterfactual predictions. Our experiments demonstrate that our models can enable effective control despite data biases.	zhiting hu	zhitinghu@gmail.com	zhiting hu (Amazon / UC San Diego)*; Li Erran Li (Amazon / Columbia University)	hu, zhiting*; Li, Li Erran	zhitinghu@gmail.com*; erranlli@gmail.com			4	2	1	50	0	Disabled (0)	Accept (oral)	Yes	Yes	No	No	Causal_framework_for_controllable_generation-2.pdf (210,382 bytes)	1		0	Aakanksha Naik (); Eric Wallace (U.C. Berkeley)	anaik@cs.cmu.edu; ericwallace@berkeley.edu	Eric Wong (MIT)	wongeric@mit.edu			N/A
90	3/6/2021 12:54:41 AM -08:00	3/6/2021 3:56:42 AM -08:00	On Strength and Transferability of Adversarial Examples: Stronger Attack Transfers Better	Our work revisits adversarial attack by perceiving it as shifting the sample semantically close to or far from a certain class, i.e. interest class. With this perspective, we introduce a new metric called interest class rank (ICR), i.e. the rank of interest class in the adversarial example, to evaluate adversarial strength. The widely used attack success rate (ASR) only taking the top-1 prediction into account can be seen as a special case of ICR. Considering top-k prediction, our ICR constitutes a fine-grained evaluation metric and it can also be readily extended to transfer-based black-box attack. With the widely observed phenomenon that I-FGSM transfers worse than FGSM, adversarial transferability, i.e. attack strength on the black-box target model, is widely reported to be at odds with white-box attack strength. Our work challenges this widely held belief with the finding that increasing the number of iterations boosts both white-box strength and black-box transferability. This finding provides a non-trivial insight that adversarial transferability can be enhanced through improving the white-box adversarial strength. To this end, we provide a geometric perspective on the logit gradient and propose a new loss that achieves SOTA white-box attack strength, consequently, also leading to SOTA attack strength in the black-box setting. 	Chaoning Zhang	chaoningzhang1990@gmail.com	Chaoning Zhang (KAIST)*; Philipp Benz (KAIST); Adil Karjauv (KAIST); In So Kweon (KAIST)	Zhang, Chaoning*; Benz, Philipp; Karjauv, Adil; Kweon, In So	chaoningzhang1990@gmail.com*; pbenz@kaist.ac.kr; mikolez@gmail.com; iskweon77@kaist.ac.kr			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	Semantically_Stronger_Attack_Transfers_Better (2).pdf (624,549 bytes)	1		0	Francesco Croce (University of Tübingen); Sven Gowal (DeepMind)	francesco91.croce@gmail.com; sgowal@google.com	Eric Wong (MIT)	wongeric@mit.edu			This paper is under review for ICML2021
91	3/6/2021 1:12:15 AM -08:00	3/6/2021 1:15:35 AM -08:00	Online Dataset Drift in O(log N)	Dataset drift is a major issue for security models. We present an online method for tracking drift that takes $O(\log(n))$ per sample. This is accomplished by using the geometric properties of  covertrees to divide and conquer the problem. We also present a library that implements the method, and some future directions that are now open due to the extreme speedup over the current  methods of measuring dataset drift like Wasserstein.	Stephen E Cattell	scattell@gmail.com	Stephen E Cattell (Elastic)*	Cattell, Stephen E*	scattell@gmail.com*			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	iclr2021_conference.pdf (4,670,047 bytes)	1		0	Li Erran Li (Amazon / Columbia University); Payel Das (IBM Research)	erranlli@gmail.com; daspa@us.ibm.com	Eric Wong (MIT)	wongeric@mit.edu			It has not.
92	3/6/2021 2:21:47 AM -08:00	3/6/2021 4:05:36 AM -08:00	Batch Normalization Increases Adversarial Vulnerability and Decreases Adversarial Transferability: A Feature Perspective	Adversarial examples constitute a threat against modern deep neural networks (DNNs). Despite numerous explorations on image-dependent adversarial perturbation (DAP), the investigation on universal adversarial perturbation (UAP) is relatively limited. The universal attack can be seen as a more practical attack because the perturbation can be generated beforehand and applied directly during the attack stage. How to generate UAP without access to the training data remains an open problem. In this work, we attempt to address this issue progressively. First, we propose a self-supervision loss to alleviate the need for ground-truth labels with the assumption that it is easier to get access to a training dataset without labels. Second, we attempt to address this issue by utilizing a very small amount of images. Our results show that our simple approach outperforms previous work by a large margin. Third, we attempt to generate a data-free UAP, i.e. without access to the training dataset at all. To this end, we propose to utilize artificial jigsaw images as the proxy dataset, and our approach outperforms existing methods by a large margin. 	Philipp Benz	pbenz@kaist.ac.kr	Philipp Benz (KAIST)*; Chaoning Zhang (KAIST); Adil Karjauv (KAIST); In So Kweon (KAIST)	Benz, Philipp*; Zhang, Chaoning; Karjauv, Adil; Kweon, In So	pbenz@kaist.ac.kr*; chaoningzhang1990@gmail.com; mikolez@gmail.com; iskweon77@kaist.ac.kr			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR2021_Batch_Normalization_Increases_Adversarial_Vulnerability.pdf (1,757,426 bytes)	1		0	Apostolos Modas (EPFL); Michael Everett (MIT)	apostolos.modas@epfl.ch; mfe@mit.edu	Eric Wong (MIT)	wongeric@mit.edu			It will be submited to ICCV2021
93	3/6/2021 2:23:57 AM -08:00	3/6/2021 2:29:41 AM -08:00	Expected Tight Bounds for Robust Training	Training deep neural networks that are robust to norm-bounded adversarial attacks remains an elusive problem. While exact and inexact verification-based methods are generally too expensive to train large networks, it was demonstrated that bounded input intervals can be inexpensively propagated from a layer to another through deep networks. This interval bound propagation approach (IBP) not only has improved both robustness and certified accuracy but was the first to be employed on large/deep networks. However, due to the very loose nature of the IBP bounds, the required training procedure is complex and involved. In this paper, we closely examine the bounds of a block of layers composed in the form of Affine-ReLU-Affine. To this end, we propose expected tight bounds (true bounds in expectation), referred to as ETB, which are provably tighter than IBP bounds in expectation. We then extend this result to deeper networks through blockwise propagation and show that we can achieve orders of magnitudes tighter bounds compared to IBP. Furthermore, using a simple standard training procedure, we can achieve an impressive robustness-accuracy trade-off on both MNIST and CIFAR10.	Modar Alfadly	Modar.Alfadly@kaust.edu.sa	Salman Alsubaihi (KAUST); Adel Bibi (University of Oxford); Modar Alfadly (KAUST)*; Abdullah J Hamdi (KAUST); Bernard Ghanem (KAUST)	Alsubaihi, Salman; Bibi, Adel; Alfadly, Modar*; Hamdi, Abdullah J; Ghanem, Bernard	salman.subaihi@kaust.edu.sa; adel.bibi@kaust.edu.sa; Modar.Alfadly@kaust.edu.sa*; abdullah.hamdi@kaust.edu.sa; Bernard.Ghanem@kaust.edu.sa			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLRw21_RobustML_ETB.pdf (2,806,975 bytes)	1		0	Dimitris Tsipras (MIT); Hadi Salman (MIT)	tsipras@mit.edu; hady@mit.edu	Eric Wong (MIT)	wongeric@mit.edu			
94	3/6/2021 2:48:00 AM -08:00	3/6/2021 3:51:00 AM -08:00	ALT-MAS: A Data-Efficient Framework for Active Testing of Machine Learning Algorithms	Machine learning models are being used extensively in many important areas, but there is no guarantee that a model will always perform well or as its developers intended. Understanding the correctness of a model is crucial to prevent potential failures that may have significant detrimental impact in critical application areas. In this paper, we propose a novel framework to efficiently test a machine learning model using only a small amount of labelled test data. The key idea is to efficiently estimate the metrics of interest for a model-under-test using Bayesian neural network (BNN). We develop a novel data augmentation method that helps to train the BNN to achieve high accuracy. We also devise a theoretic information based sampling strategy to sample data points so as to achieve accurate estimations for the metrics of interest. Finally, we conduct an extensive set of experiments to test various machine learning models for different types of metrics. Our experiments show that given a testing budget, the estimation of the metrics by our method is significantly better compared to existing baselines.	Huong Ha	huong.ha@rmit.edu.au	Huong Ha (RMIT University)*; Sunil Gupta (Deakin University, Australia); Santu Rana (Deakin University, Australia); Svetha Venkatesh (Deakin University)	Ha, Huong*; Gupta, Sunil; Rana, Santu; Venkatesh, Svetha	huong.ha@rmit.edu.au*; sunil.gupta@deakin.edu.au; santu.rana@deakin.edu.au; svetha.venkatesh@deakin.edu.au			0	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ALTMAS_RobustML_ICLRSubmission.pdf (1,706,493 bytes)	1		0	Jessica Yung (Google Research); Kuan-Hao Huang (University of California, Los Angeles)	j.yung357@gmail.com; khhuang@cs.ucla.edu	Eric Wong (MIT)	wongeric@mit.edu			No.
95	3/6/2021 3:13:08 AM -08:00	3/6/2021 4:05:09 AM -08:00	Class-conditioned Domain Generalization via Wasserstein Distributional Robust Optimization	Given multiple source domains, domain generalization aims at learning a universal model that performs well on any unseen but related target domain. In this work, we focus on the domain generalization scenario where domain shifts occur among class-conditional distributions of different domains. Existing approaches are not sufficiently robust when the variation of conditional distributions given the same class is large. In this work, we extend the concept of distributional robust optimization to solve the class-conditional domain generalization problem. Our approach optimizes the worst-case performance of a classifier over class-conditional distributions within a Wasserstein ball centered around the barycenter of the source conditional distributions. We also propose an iterative algorithm for learning the optimal radius of the Wasserstein balls automatically. Experiments show that the proposed framework has better performance on unseen target domain than approaches without domain generalization.	Jingge Wang	wangjg19@mails.tsinghua.edu.cn	Jingge Wang (Tsinghua University)*; Yang Li (Tsinghua-Berkeley Shenzhen Institute, Tsinghua University); Liyan Xie (Georgia Tech); Yao Xie (Georgia Tech)	Wang, Jingge*; Li, Yang; Xie, Liyan; Xie, Yao	wangjg19@mails.tsinghua.edu.cn*; yangli@sz.tsinghua.edu.cn; lxie49@gatech.edu; yao.xie@isye.gatech.edu			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR_2021_Conference_Workshop_v2 (5).pdf (271,810 bytes)	1		0	Deyi Liu (University of North Carolina); Rohan Taori (Stanford University)	deyi@live.unc.edu; rtaori@stanford.edu	Eric Wong (MIT)	wongeric@mit.edu			No.
97	3/6/2021 3:55:32 AM -08:00	3/6/2021 4:11:37 AM -08:00	Stochastic Depth Boosts Transferability of Non-Targeted and Targeted Adversarial Attacks	Deep Neural Networks (DNNs) are widely known to be vulnerable to adversarial examples which have a surprising property of being transferable (or generalizable) to unknown networks.  This property has been exploited in numerous works for achieving transfer-based black-box attacks.  In contrast to most existing works that manipulate the image input for boosting transferability, our work manipulates the model architecture. Specifically, we boost the transferability with stochastic depth by randomly removing a subset of layers in networks with skip connections.Technical-wise, our proposed approach is mainly inspired by previous work improving the network generalization with stochastic depth.  Motivation-wise, our approach of removing residual module instead of skip connection is inspired by the known finding that transferability of adversarial examples are positively related to local linearity of DNNs. The experimental results demonstrate that our approach outperforms existing methods by a large margin, resulting in SOTA performance,for both non-targeted and targeted attacks. Moreover, our approach is also complementary to the existing input manipulation approaches, combined with which the performance can be further boosted.	Chaoning Zhang	chaoningzhang1990@gmail.com	Chaoning Zhang (KAIST)*; Philipp Benz (KAIST); Adil Karjauv (KAIST); In So Kweon (KAIST)	Zhang, Chaoning*; Benz, Philipp; Karjauv, Adil; Kweon, In So	chaoningzhang1990@gmail.com*; pbenz@kaist.ac.kr; mikolez@gmail.com; iskweon77@kaist.ac.kr			1	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	_ICLR_2021_Workshop__Boosting_Adversarial_Transferability_Through_Stochastic_Depth (4).pdf (114,773 bytes)	1		0	Apostolos Modas (EPFL); Huan Zhang (UCLA)	apostolos.modas@epfl.ch; huanzhang@ucla.edu	Eric Wong (MIT)	wongeric@mit.edu			N/A
98	3/6/2021 3:57:15 AM -08:00	3/6/2021 4:07:12 AM -08:00	RATT: Leveraging Unlabeled Data to Guarantee Generalization	To assess generalization, machine learning scientists typically either (i) bound the generalization gap and, after training, use the empirical risk to obtain a bound on the true risk; or (ii) validate empirically on holdout data. However, (i) typically yields vacuous guarantees for overparameterized models. Furthermore, (ii) shrinks the training set and its guarantee erodes with each re-use of the holdout set. In this paper, we introduce a method that leverages unlabeled data to produce non-vacuous bounds. After augmenting our (labeled) training set with randomly labeled fresh examples, we train in the standard fashion. Because models tend to fit true labels before noisy labels, we reach a point where the error on the clean training data is low but the error on the noisy training data is high. Our bound translates the (high) error on the noisy data to a (tight) post-hoc upper bound on the true risk. Theoretically, we prove that our bound holds with 0-1 loss minimization and with gradient-descent-trained linear classifiers. Empirically, for deep networks applied to canonical computer vision and NLP tasks, our bound provides non-vacuous generalization guarantees that track actual performance closely. This work provides practitioners with an option for certifying the generalization of deep nets even when unseen labeled data is unavailable and provides theoretical insights into the relationship between random label noise and generalization.	Saurabh Garg	sgarg2@andrew.cmu.edu	Saurabh Garg (CMU)*; Sivaraman Balakrishnan (CMU); Zico Kolter (Carnegie Mellon University); Zachary  Lipton (Carnegie Mellon University)	Garg, Saurabh*; Balakrishnan, Sivaraman; Kolter, Zico; Lipton, Zachary 	sgarg2@andrew.cmu.edu*; the.seeing.stone@gmail.com; zkolter@cs.cmu.edu; zlipton@cmu.edu			0	2	1	50	0	Disabled (0)	Accept	Yes	Yes	No	No	ICLR_2021__workshop___Post_hoc_generalization_bound.pdf (561,700 bytes)	1		0	Aakanksha Naik (); Shiori Sagawa (Stanford University)	anaik@cs.cmu.edu; ssagawa@cs.stanford.edu	Eric Wong (MIT)	wongeric@mit.edu			No
99	3/6/2021 3:58:30 AM -08:00	3/6/2021 4:05:17 AM -08:00	Towards Data-free Universal Adversarial Perturbations with Artificial Jigsaw Images	Adversarial examples constitute a threat against modern deep neural networks (DNNs). Despite numerous explorations on image-dependent adversarial perturbation (DAP), the investigation on universal adversarial perturbation (UAP) is relatively limited. The universal attack can be seen as a more practical attack because the perturbation can be generated beforehand and applied directly during the attack stage. How to generate UAP without access to the training data remains an open problem. In this work, we attempt to address this issue progressively. First, we propose a self-supervision loss to alleviate the need for ground-truth labels with the assumption that it is easier to get access to a training dataset without labels. Second, we attempt to address this issue by utilizing a very small amount of images. Our results show that our simple approach outperforms previous work by a large margin. Third, we attempt to generate a data-free UAP, i.e. without access to the training dataset at all. To this end, we propose to utilize artificial jigsaw images as the proxy dataset, and our approach outperforms existing methods by a large margin.	Chaoning Zhang	chaoningzhang1990@gmail.com	Chaoning Zhang (KAIST)*; Philipp Benz (KAIST); Adil Karjauv (KAIST); Jae Won Cho (KAIST); In So Kweon (KAIST)	Zhang, Chaoning*; Benz, Philipp; Karjauv, Adil; Cho, Jae Won; Kweon, In So	chaoningzhang1990@gmail.com*; pbenz@kaist.ac.kr; mikolez@gmail.com; chojw2017@gmail.com; iskweon77@kaist.ac.kr			1	2	2	100	0	Disabled (0)	Accept	Yes	Yes	No	No	towards_data_free_uap.pdf (1,757,474 bytes)	1		0	Elan Rosenfeld (Carnegie Mellon University); Leslie Rice (Carnegie Mellon University)	ekr@andrew.cmu.edu; larice@cs.cmu.edu	Kai-Wei Chang (UCLA)	kw@kwchang.net			N/A
100	3/8/2021 9:13:46 AM -08:00	3/30/2021 9:49:33 AM -07:00	On Calibration and Out-of-Domain Generalization	Out-of-domain (OOD) generalization is a significant challenge for machine learning models. To overcome it, many novel techniques have been proposed, often focused on learning models with certain invariance properties. In this work, we draw a link between OOD performance and model calibration, arguing that calibration across multiple domains can be viewed as a special case of an invariant representation leading to better OOD generalization. Specifically, we prove in a simplified setting that models which achieve multi-domain calibration are free of spurious correlations. Using datasets from the recently proposed WILDS OOD benchmark \cite{koh2020wilds} we demonstrate that re-calibrating models across multiple domains in a validation set improves performance on unseen test domains. We believe this connection between calibration and OOD generalization is promising from a practical point of view and deserves further research from a theoretical point of view.	Yoav Wald	yoav.wald@gmail.com	Yoav Wald (Hebrew University)*; Amir Feder (Technion); Daniel Greenfeld (Tel Aviv University,$Wiezmann Institute of Science); Uri Shalit (Technion)	Wald, Yoav*; Feder, Amir; Greenfeld, Daniel; Shalit, Uri	yoav.wald@gmail.com*; amirfeder@gmail.com; danielgreenfeld3@gmail.com; urishalit@technion.ac.il			0	2	2	100	0	Disabled (0)	Accept (oral)	Yes	Yes	No	No	out_of_domain_calibration_iclr_workshop.pdf (405,605 bytes)	1		0	Olivia Wiles (DeepMind); Rohan Taori (Stanford University)	ow@robots.ox.ac.uk; rtaori@stanford.edu	Yonatan Belinkov (Technion)	belinkov@mit.edu			
101	4/15/2021 1:48:19 AM -07:00	4/16/2021 9:46:35 AM -07:00	TextFlint: Uniﬁed Multilingual Robustness Evaluation Toolkit for  Natural Language Processing	Various robustness evaluation methodologies from different perspectives have been proposed for different natural language processing (NLP) tasks, which often focus on either universal or task-speciﬁc generalization capabilities. TextFlint is a multilingual robustness evaluation toolkit for NLP tasks that incorporates universal text transformation, task-speciﬁc transformation, adversarial attack, subpopulation, and their combinations to provide comprehensive robustness analysis. It enables practitioners to automatically evaluate their models from various aspects or to customize their evaluations as desired with just a few lines of code, and generates complete analytical reports as well as targeted augmented data to address the shortcomings of the model’s robustness. To guarantee user acceptability, all the text transformations are linguistically based and passed human evaluation. To validate the utility, we performed large-scale empirical evaluations (over 67,000) on state-of-the-art deep learning models, classic supervised methods, and real-world systems. TextFlint is already available at https://github.com/textflint, with all the evaluation results demonstrated at textflint.io.	Tao Gui	tgui16@fudan.edu.cn	Xiao Wang (Fudan University); Qin Liu (Fudan University); Tao Gui (Fudan University)*; Qi Zhang (Fudan University); Xuanjing Huang (" Fudan University, China")	Wang, Xiao; Liu, Qin; Gui, Tao*; Zhang, Qi; Huang, Xuanjing	xiao_wang20@fudan.edu.cn; liuq19@fudan.edu.cn; tgui16@fudan.edu.cn*; qz@fudan.edu.cn; xjhuang@fudan.edu.cn			0	0	0	0	0	Disabled (0)	Accept	No	No	No	No	ICLRworkshop__TextFlint_.pdf (410,647 bytes)	1		0							
